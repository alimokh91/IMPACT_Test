{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICOM FlowMRI Writer - output of DICOM files from hpc-predict-io HDF5-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Notebook parameters - to be moved into papermill parameters\n",
    "hpc_predict_data_root = \"/home/lukasd/src/hpc-predict/hpc-predict/data/v1\" # e.g. ../../../data/v0\n",
    "mri_data_root = \"input_data/original/mri/MRT Daten Bern tar\"\n",
    "mri_samples = ['10/10-10000718-10000719.h5']\n",
    "tar_files = True\n",
    "mri_preprocessed_root = \"input_data/preprocessed/mri/MRT Daten Bern tar\"\n",
    "output_root = \"output_data/clinical/testing/dicom-io/MRT Daten Bern\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10/10-10000718-10000719.h5'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bash version of list\n",
    "mri_samples_bash = ' '.join([str(sample) for sample in mri_samples])\n",
    "tar_files_bash = '--tar' if tar_files else ''\n",
    "mri_samples_bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: hpc_predict_data_root=/home/lukasd/src/hpc-predict/hpc-predict/data/v1\n",
      "env: mri_data_root=input_data/original/mri/MRT Daten Bern tar\n",
      "env: mri_samples=10/10-10000718-10000719.h5\n",
      "env: tar_files=--tar\n",
      "env: mri_preprocessed_root=input_data/preprocessed/mri/MRT Daten Bern tar\n",
      "env: output_root=output_data/clinical/testing/dicom-io/MRT Daten Bern\n"
     ]
    }
   ],
   "source": [
    "%env hpc_predict_data_root = {hpc_predict_data_root}\n",
    "%env mri_data_root = {mri_data_root}\n",
    "%env mri_samples = {mri_samples_bash}\n",
    "%env tar_files = {tar_files_bash}\n",
    "%env mri_preprocessed_root = {mri_preprocessed_root}\n",
    "%env output_root = {output_root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_sources = [sample.split('/', maxsplit=1)[0] for sample in mri_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script assumes that you have the Freiburg dataset available, otherwise first run `data/fetch_scripts/fetch_freiburg_to_tar.sh data/tmp data/v1`.\n",
    "\n",
    "To run the following steps, the DICOM headers must previously be converted to pandas tables, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# set -x\n",
    "# python3  convert_dicom_headers_to_pandas_df.py --hpc-predict-data-root \"${hpc_predict_data_root}\" --mri-data-root \"${mri_data_root}\" --mri-samples ${mri_samples} ${tar_files} --output-root \"${output_root}\"\n",
    "# set +x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import sys\n",
    "import json\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "if not '../../python' in sys.path:\n",
    "    sys.path.append('../../python')\n",
    "from mr_io import FlowMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Possibly duplicate with code in convert_dicom_headers_to_pandas_df\n",
    "\n",
    "if os.path.exists(os.path.join(hpc_predict_data_root, 'encrypt')):\n",
    "    hpc_predict_data_root = os.path.join(hpc_predict_data_root, 'decrypt')\n",
    "    assert os.path.exists(hpc_predict_data_root)\n",
    "\n",
    "mri_data_root = os.path.join(hpc_predict_data_root, mri_data_root)\n",
    "assert os.path.exists(mri_data_root)\n",
    "\n",
    "for sample in mri_sources:\n",
    "    assert os.path.exists(os.path.join(mri_data_root, str(sample) + ('.tar' if tar_files else '/') ))\n",
    "    \n",
    "if tar_files:\n",
    "    mri_source_tar_paths = [os.path.relpath(os.path.join(mri_data_root, str(sample) + '.tar'), start=hpc_predict_data_root) for sample in mri_sources]\n",
    "\n",
    "mri_preprocessed_root = os.path.join(hpc_predict_data_root, mri_preprocessed_root)\n",
    "assert os.path.exists(mri_preprocessed_root)\n",
    "\n",
    "for sample in mri_samples:\n",
    "    assert os.path.exists(os.path.join(mri_preprocessed_root, sample))\n",
    "    assert os.path.exists(os.path.join(mri_preprocessed_root, sample.replace('.h5','.json')))\n",
    "\n",
    "output_root = os.path.join(hpc_predict_data_root, output_root)\n",
    "assert os.path.exists(output_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read FlowMRIs (and potentially original DICOM files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_mris = []\n",
    "\n",
    "for sample in mri_samples:\n",
    "    flow_mri = FlowMRI.read_hdf5(os.path.join(mri_preprocessed_root, sample))\n",
    "    with open(os.path.join(mri_preprocessed_root, sample.replace('.h5','.json')), 'r') as json_f:\n",
    "        metainfo = json.load(json_f)\n",
    "    flow_mris.append({ 'h5' : flow_mri,\n",
    "                       'json': metainfo })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flow_mris[0]['json']['file_path']['P_files'][0])\n",
    "len(flow_mris[0]['json']['file_path']['M_files'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion to DICOM from hpc-predict-io HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine target folder\n",
    "output_sample_dir = os.path.join(output_root, sample.rstrip('.h5'))\n",
    "os.makedirs(output_sample_dir, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with highdicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with pydicom-seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with pure pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice_location = geometry[2]\n",
    "# calculate image position patient from old_image_position_origin + (geometry[2] - slice_location[0])*ort_unit\n",
    "# Pixel Spacing = (geometry[i][-1] - geometry[i][0])/(len(geometry[i])-1)\n",
    "# instance number: magnitude: t*num_slice_locations + z + 1, \n",
    "#                  phase: j*cardiac_number_of_images*num_slice_locations + t*num_slice_locations + z + 1\n",
    "# trigger times\n",
    "# Rows = len(geometry[0]) ?\n",
    "# Columns = = len(geometry[0]) ?\n",
    "# Rescale Intercept/Slope ? Intercept = -max(abs(velocity_mean)), slope = 2*abs(Intercept)/2*Bits Stored(?)\n",
    "# skip all other tags with multiplicity > 1 such as acquisition/content time (make up SOP Class UID etc. with uid.generate_uid)\n",
    "# for those with multiplicity 1 select which ones to add \n",
    "#\n",
    "# Create tags to annotate HPC-PREDICT information, such as code that was used to produce the data\n",
    "#\n",
    "# Question: how do we add overlays? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_native_grouped_unique_flow = df_native_grouped_unique[(df_native_grouped_unique['Study Instance UID'] == flow_mris[1]['study_instance_uid']) & (df_native_grouped_unique['Sequence Name'] == flow_mris[1]['sequence_name'])]\n",
    "df_native_grouped_unique_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_native_grouped_unique_flow_reindexed = df_native_grouped_unique_flow.set_index(df_native_grouped_unique_flow['Image Type'].apply(lambda x: x[0][2]))\n",
    "# df_native_grouped_unique_flow_reindexed.loc['M']['FilePath'][0]\n",
    "\n",
    "singleton_labels = df_native_grouped_unique_flow.drop(df_native_grouped_unique_flow.columns[~df_native_grouped_unique_flow.applymap(lambda x: not(hasattr(x,'__len__') and len(x) == 1) ).all()], axis=1) #.columns.values\n",
    "singleton_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_series_keys = ['FileCollectionID', 'Instance Creation Date', 'Patient ID', 'Study Instance UID', 'Sequence Name', 'Series Instance UID', 'Image Type']\n",
    "# df.groupby(group_by_series_keys)\n",
    "\n",
    "# Get study instance UID, convert to pandas table, compute unique tags by base tag address, iterate over tags - for unique copy them to new dataset except if should be included (by second table tag -> name), for non-unique, replace them appropriately (name look-up, etc.) \n",
    "\n",
    "tag_to_name = df.apply( lambda c: c.dropna().apply(lambda x: x.name).unique(), axis=0).transpose()\n",
    "name_to_tag = tag_to_name.reset_index().rename(columns={0: 'name', 'index': 'tag'}).set_index('name')\n",
    "\n",
    "tag_to_name[0]\n",
    "name_to_tag['tag']['Study Instance UID']\n",
    "#df[df.apply(lambda row: row['Study Instance UID'] == study_instance_uid and row['Sequence Name'] == sequence_name, axis=1)]\n",
    "\n",
    "# df_hashified = df.applymap(lambda x: unpack_pydicom_value(x.value) if pd.notnull(x) else x)\n",
    "# df_tag_to_name = df.applymap(lambda x: x.name if pd.notnull(x) else x)\n",
    "# df_hashified.groupby(group_by_series_keys)\n",
    "\n",
    "# # df_native_grouped = df_native_renamed.groupby(group_by_series_keys)\n",
    "# # df_native_grouped.apply(lambda x: agg_unique(x))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicom_img[name_to_tag['tag']['SOP Class UID']]\n",
    "pydicom.dcmread(df_native_grouped_unique_flow_reindexed.loc['M']['FilePath'][0]).file_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_native_grouped_unique_flow_reindexed.loc['M','Specific Character Set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_meta = FileMetaDataset\n",
    "# file_meta = pydicom.dataset.FileMetaDataset()\n",
    "# file_meta.MediaStorageSOPClassUID = dicom_img.file_meta[0x00020002].value\n",
    "# file_meta.ImplementationClassUID = PYDICOM_IMPLEMENTATION_UID\n",
    "\n",
    "# First get the study (flow MRI needs to describe data it comes from...)\n",
    "\n",
    "# for series in source study, open first image as dicom_img\n",
    "dcm_img = pydicom.dcmread(df_native_grouped_unique_flow_reindexed.loc['M']['FilePath'][0])\n",
    "# create file_meta_ds/file_ds\n",
    "ds = pydicom.dataset.Dataset()\n",
    "\n",
    "print(\"### Simulating writing of DICOM ###\")\n",
    "for el in dcm_img: # also include dcm_img.file_meta later\n",
    "    print(el.name, flush=True)\n",
    "    if el.name == 'Pixel Data':\n",
    "        print(\"Writing Pixel Data\") # replace dcm-che prefix by pydicom prefix        \n",
    "    elif len(df_native_grouped_unique_flow_reindexed.loc['M', el.name]) == 1: # FIXME: use el.tag instead of el.name (compute unique values by tags)\n",
    "        if not el.name in ['Referenced Image Sequence', 'Study Instance UID', 'Series Instance UID', 'Rows', 'Columns', 'Pixel Spacing']: # more? for UIDs can just replace dcm-che prefix by that of pydicom implementation\n",
    "            print(\"Copying unique element {} \".format(el.name)) # replace dcm-che prefix by pydicom prefix\n",
    "#             file_ds.add(el.tag, el.VR, el.value) # module modifications...\n",
    "        else: # special treatment of el.tag\n",
    "            print(\"Special treatment for {} \".format(el.name)) # replace dcm-che prefix by pydicom prefix\n",
    "    else: # probably one of the tags that has to be computed... (see above for the manual)\n",
    "        print(\"Element {} has different values\".format(el.name))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load pickled DataFrames and metainformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(hpc_predict_data_root, mri_data_root,\"venc.json\"), 'r') as f:\n",
    "    venc = json.load(f)\n",
    "venc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pkls = sorted([os.path.join(output_root, str(sample), \"dicom_header_df.pkl\") for sample in mri_samples], key=lambda x: int(os.path.basename(os.path.dirname(x))) ) # os.path.basename(x).split('.')[0]\n",
    "pkls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pandas DataFrame that contains pydicom DataElement objects in individual entries \n",
    "df = pd.concat([pd.read_pickle(pkl) for pkl in pkls],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# A pydicom Dataelement: The value is held in the _value attribute, \n",
    "# often in \"semi-serialized\" representation with the VR describing the DICOM type  \n",
    "vars(df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Use names instead of DICOM tags as column labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Compute the DICOM tag -> name mapping to rename columns\n",
    "tag_names = df.apply( lambda c: c.dropna().apply(lambda x: x.name).unique(), axis=0); assert tag_names.shape[0] == 1; tag_names = tag_names.loc[0] \n",
    "#tag_names.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = df.rename(columns=tag_names.to_dict())\n",
    "\n",
    "# display only values\n",
    "pd.set_option('display.max_columns', 20)\n",
    "df_renamed.applymap(lambda x: x.value if pd.notnull(x) else x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Determine range of DICOM tags (via unique rows, grouped by patient ID > study UID > series UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine types of DataElement.value entries\n",
    "from collections import namedtuple, OrderedDict\n",
    "from types import SimpleNamespace\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.concat([pd.Series(name=name, data=col.dropna().unique()) for (name, col) in df_renamed.applymap(lambda x: (x.VR, type(x.value)) if pd.notnull(x) else x).iteritems()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract native Python datatypes (built-in/standard library) from pydicom DataElements\n",
    "def get_native_from_pydicom(val):\n",
    "    from pydicom.valuerep import DA, DT, TM, DSfloat, DSdecimal, IS, PersonName\n",
    "    from pydicom.uid import UID\n",
    "    from pydicom.multival import MultiValue\n",
    "    from datetime import (date, datetime, time, timedelta, timezone)\n",
    "    from decimal import Decimal\n",
    "\n",
    "    if isinstance(val, DA):\n",
    "        return date(val.year, val.month, val.day)\n",
    "    elif isinstance(val, DT):\n",
    "        return datetime(val.year, val.month, val.day,\n",
    "                        val.hour, val.minute, val.second,\n",
    "                        val.microsecond, val.tzinfo)\n",
    "    elif isinstance(val, TM):\n",
    "        return time(val.hour, val.minute, val.second,\n",
    "                    val.microsecond)\n",
    "    elif isinstance(val, DSfloat):\n",
    "        return float(val)\n",
    "    elif isinstance(val, DSdecimal):\n",
    "        return Decimal(val)\n",
    "    elif isinstance(val, IS):\n",
    "        return int(val)\n",
    "    elif isinstance(val, MultiValue):\n",
    "        return tuple([get_native_from_pydicom(el) for el in val])\n",
    "    elif isinstance(val, UID):\n",
    "        return str(val)\n",
    "    elif isinstance(val, (PersonName, str, int, float)):\n",
    "        return val\n",
    "    elif isinstance(val, list):\n",
    "        return tuple(val)\n",
    "    else:\n",
    "        return val        \n",
    "\n",
    "def unpack_pydicom_value(value):\n",
    "    if hasattr(type(value),'__module__') and type(value).__module__.startswith('pydicom'):\n",
    "        return get_native_from_pydicom(value)\n",
    "    elif isinstance(value, list):\n",
    "        return tuple([unpack_pydicom_value(v) for v in value])\n",
    "    elif isinstance(value,OrderedDict):\n",
    "        return tuple( [unpack_pydicom_value(v.value) for v in value.values()] )\n",
    "    elif isinstance(value, dict):\n",
    "        return tuple(value.items())\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "def agg_unique(group):\n",
    "    labels = []\n",
    "    row = []\n",
    "    for (name, col) in group[group.columns[:]].iteritems():\n",
    "        labels.append(name)\n",
    "        row.append(col.dropna().unique())\n",
    "\n",
    "    return pd.DataFrame([row], columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_native_renamed = df_renamed.applymap(lambda x: unpack_pydicom_value(x.value) if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-by patient/study/series ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_series_keys = ['FileCollectionID',\n",
    "                        'Instance Creation Date',\n",
    "                        'Patient ID',\n",
    "                        'Study Instance UID',\n",
    "                        'Sequence Name',\n",
    "                        'Series Instance UID',\n",
    "                        'Image Type']\n",
    "\n",
    "df_native_grouped = df_native_renamed.groupby(group_by_series_keys)\n",
    "df_native_grouped_unique = df_native_grouped.apply(lambda x: agg_unique(x))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All tags with <= 1 unique values per series in any series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_native_grouped_unique_singleton_cols = df_native_grouped_unique.applymap(lambda x: len(x) <= 1).all()\n",
    "df_native_grouped_unique.drop(\n",
    "    df_native_grouped_unique.columns[~df_native_grouped_unique_singleton_cols], axis=1).applymap(lambda x: x[0] if len(x) == 1 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...and number of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_native_grouped_unique.drop(\n",
    "    df_native_grouped_unique.columns[df_native_grouped_unique_singleton_cols],axis=1).applymap(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All tags with more than a single unique values in at least one series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_native_grouped_unique.drop(\n",
    "    df_native_grouped_unique.columns[df_native_grouped_unique_singleton_cols],axis=1).applymap(\n",
    "    lambda x: x[0] if len(x) == 1 else x).drop(['FilePath'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
