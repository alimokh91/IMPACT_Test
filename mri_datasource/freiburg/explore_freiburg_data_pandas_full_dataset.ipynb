{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis with Pandas - dataset-wide analysis of per-series unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpc_predict_data_dir=\"/home/lukasd/src/hpc-predict/data/v0\" # e.g. ../../../data/v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script assumes that you have the Freiburg dataset available, otherwise first run `data/fetch_scripts/fetch_freiburg.sh`.\n",
    "\n",
    "To run the following steps, the DICOM headers must previously be converted to pandas tables, i.e.\n",
    "\n",
    "```\n",
    "python convert_dicom_to_pandas.py --mri-data-root \"${hpc_predict_data_dir}/input_data/original/mri/MRT Daten Bern\" --mri-samples ... --output-root \"${hpc_predict_data_dir}/input_data/preprocessed/mri/MRT Daten Bern DICOM Header\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then extract per DICOM series unique values by running `extract_unique_tags_pandas.py` on serialized pandas `DataFrame`s:\n",
    "\n",
    "```\n",
    "python extract_unique_tags_pandas.py --headers-root \"${hpc_predict_data_dir}/input_data/preprocessed/mri/MRT Daten Bern DICOM Header\" --mri-samples ... --output-root \"${hpc_predict_data_dir}/input_data/preprocessed/mri/MRT Daten Bern DICOM Header Unique\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse extracted unique DICOM header lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pydicom\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import re\n",
    "\n",
    "pkls = sorted(glob(hpc_predict_data_dir + \"/input_data/preprocessed/mri/MRT Daten Bern DICOM Header Unique/*.pkl\"), key=lambda x: int(os.path.basename(x).split('.')[0]) )\n",
    "pkls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df_unique = []\n",
    "df_skipped = []\n",
    "for pkl in pkls:\n",
    "    read_pkl = pd.read_pickle(pkl)\n",
    "    labels, counts = np.unique(read_pkl.columns.values, return_counts=True)\n",
    "    print(\"Dropping {} from {}\".format(labels[counts > 1], os.path.basename(pkl)))\n",
    "    read_pkl = read_pkl.drop(labels=labels[counts > 1], axis=1)\n",
    "\n",
    "    read_pkl_deidentified = read_pkl['De-identification Method Code Sequence'].apply(lambda x: hasattr(x,'__len__') and len(x) == 1 and hasattr(x[0],'__len__') and len(x[0]) > 0)\n",
    "    if (~read_pkl_deidentified).sum() > 0:\n",
    "        print(\"WARNING: Found {} series in studies {} in {} without de-identification tags - skipping them!\".format(read_pkl_deidentified.sum(), list(read_pkl[~read_pkl_deidentified]['Study Instance UID'].apply(lambda x: x[0]).unique()), os.path.basename(pkl)))\n",
    "        df_skipped.append(read_pkl[~read_pkl_deidentified])\n",
    "    read_pkl = read_pkl[read_pkl_deidentified]\n",
    "    \n",
    "    df_unique.append(read_pkl)\n",
    "\n",
    "df_unique = pd.concat(df_unique, axis=0)\n",
    "if len(df_skipped) > 0:\n",
    "    df_skipped = pd.concat(df_skipped, axis=0) \n",
    "else:\n",
    "    df_skipped = None\n",
    "df_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate anonymization with de-identification tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_skipped is None:\n",
    "    print(\"Checking skipped data for any non-anonymized DICOM images\")\n",
    "    df_skipped_folders = {collection: pd.read_pickle(\"/home/lukasd/src/hpc-predict/data/v0/input_data/preprocessed/mri/MRT Daten Bern DICOM Header/{}.pkl\".format(collection)) for collection in df_skipped['FileCollectionID'].apply(lambda x: x[0]).unique()}\n",
    "    for collection, df_skipped_folder in df_skipped_folders.items():\n",
    "        print(\"Evaluating FileCollectionID {} for missing de-identification tags\".format(collection))\n",
    "\n",
    "        df_skipped_folder_hashified = df_skipped_folder.rename(columns=df_skipped_folder.apply( lambda c: c.dropna().apply(lambda x: x.name).unique(), axis=0).iloc[0].to_dict())\\\n",
    "                                                       .applymap(lambda x: unpack_pydicom_value(x.value) if pd.notnull(x) else x)\n",
    "        df_skipped_folder_anonymized = df_skipped_folder_hashified['De-identification Method Code Sequence'].apply(lambda x: hasattr(x,'__len__') and len(x) > 0)\n",
    "        df_skipped_folder_unaffected = df_skipped_folder_hashified[df_skipped_folder_anonymized]\n",
    "        df_skipped_folder_affected = df_skipped_folder_hashified[~df_skipped_folder_anonymized]\n",
    "\n",
    "        print(\"Affected studies:   {} \".format(df_skipped_folder_affected['Study Instance UID'].unique()))\n",
    "        print(\"      -> folders:   {} \".format(df_skipped_folder_affected['FilePath'].apply(lambda x: '/'.join(x.split('/')[-5:-2]) ).unique()))\n",
    "        print(\"Unaffected studies: {} \".format(df_skipped_folder_unaffected['Study Instance UID'].unique()))\n",
    "        print(\"      ->   folders: {} \".format(df_skipped_folder_unaffected['FilePath'].apply(lambda x: '/'.join(x.split('/')[-5:-2]) ).unique()))\n",
    "else:\n",
    "    print(\"No skipped data - no DICOM anonymization issues w.r.t. \\'De-identification Method Code Sequence\\'-tag found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.hist(df_unique['Sequence Name'].apply(lambda x: repr(tuple(x))).values)\n",
    "plt.title('Sequence Name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_unique[df_unique['Sequence Name'] == 'fl3d1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the null-values (e.g. for debugging)\n",
    "\n",
    "# # Full dataset\n",
    "# df_unique.applymap(lambda x: len(x) if np.all(pd.notnull(x)) else x)\n",
    "\n",
    "# # Sequence-wise\n",
    "# df_unique[df_unique['Sequence Name'] == 'Nav'].applymap(lambda x: len(x) if np.all(pd.notnull(x)) else x)\n",
    "\n",
    "# # Columns with null-values (sequence-name-agnostic!)\n",
    "# df_unique.columns[df_unique.applymap(lambda x: np.isnan(x) if not isinstance(x,np.ndarray) else False).any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-specific analysis: choose sequence to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_sequence = 'fl3d1'\n",
    "df_unique_seq = df_unique[df_unique['Sequence Name'].apply(lambda x: x[0]) == analyzed_sequence]\n",
    "\n",
    "# # Truncate columns for binary search of pathological column \n",
    "# df_unique_seq = df_unique_seq.drop(np.concatenate([df_unique_seq.columns.values[:76*len(df_unique_seq.columns.values)//128],df_unique_seq.columns.values[77*len(df_unique_seq.columns.values)//128:]]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here you can add columns of interest such as Trigger Time interval divided by heart cycle period (Nominal Interval) that will be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_time_nominal_interval_ratio(row):\n",
    "    trigger_time = row['Trigger Time']\n",
    "    return (trigger_time.max()-trigger_time.min())/row['Nominal Interval'].mean()\n",
    "    \n",
    "if analyzed_sequence == 'fl3d1':\n",
    "    df_unique_seq = pd.concat([df_unique_seq, pd.DataFrame({'Derived: Trigger Time range/Nominal Interval': df_unique_seq.apply(lambda x: np.array([trigger_time_nominal_interval_ratio(x)]),axis=1),\n",
    "                                                            'Derived: Trigger Time Min': df_unique_seq.apply(lambda x: np.array([x['Trigger Time'].min()]),axis=1),\n",
    "                                                            'Derived: Nominal Interval - Trigger Time Max': df_unique_seq.apply(lambda x: np.array([x['Nominal Interval'].mean() - x['Trigger Time'].max()]),axis=1),\n",
    "                                                           })], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display columns that will be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize discarded columns computed from different conditions (can be altered) via set differences\n",
    "if analyzed_sequence == 'Nav':\n",
    "#     set_diff = set(df_unique_seq.columns[df_unique_seq.apply(lambda x: x.apply(lambda y: (isinstance(y,float) and np.isnan(y)) or (len(y) > 0 and isinstance(y[0],bytes)) or ( len(y) > 3 ) ), axis=1).any()].values) - \\\n",
    "#                set(df_unique_seq.columns[df_unique_seq.apply(lambda x: x.apply(lambda y: (isinstance(y,float) and np.isnan(y)) or ( len(y) > 3 ) ), axis=1).any()].values)\n",
    "    set_diff = set(df_unique_seq.columns[df_unique_seq.apply(lambda x: x.apply(lambda y: (hasattr(y,'__len__') and len(y) > 5 ) ), axis=1).any()].values) - \\\n",
    "               set(df_unique_seq.columns[df_unique_seq.apply(lambda x: x.apply(lambda y: False ), axis=1).any()].values)\n",
    "else:\n",
    "#     set_diff = set(df_unique_seq.columns[df_unique_seq.apply(lambda x: x.apply(lambda y: (isinstance(y,float) and np.isnan(y)) or (len(y) > 0 and isinstance(y[0],bytes)) or ( len(y) >= np.min([*x['Cardiac Number of Images'],len(x['Slice Location'])]) ) ), axis=1).any()].values) - \\\n",
    "#                set(df_unique_seq.columns[df_unique_seq.apply(lambda x: x.apply(lambda y: (isinstance(y,float) and np.isnan(y)) or ( len(y) >= np.min([*x['Cardiac Number of Images'],len(x['Slice Location'])]) ) ), axis=1).any()].values)\n",
    "    set_diff = set(df_unique_seq.columns[df_unique_seq.apply(lambda x: x.apply(lambda y: (hasattr(y,'__len__') and len(y) >= np.min([*x['Cardiac Number of Images'],len(x['Slice Location'])]) ) ), axis=1).any()].values) - \\\n",
    "               set(df_unique_seq.columns[df_unique_seq.apply(lambda x: x.apply(lambda y: False ), axis=1).any()].values)\n",
    "df_unique_seq[ [el for el in set_diff] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard the above visualized columns\n",
    "if analyzed_sequence == 'Nav':\n",
    "    # Eliminate per-image varying information, bytes\n",
    "#     df_unique_drop_columns = df_unique_seq.columns[df_unique_seq.apply(lambda x: x.apply(lambda y: (isinstance(y,float) and np.isnan(y)) or (len(y) > 0 and isinstance(y[0],bytes)) or ( len(y) > 3 ) ), axis=1).any()]\n",
    "    df_unique_too_long_columns = df_unique_seq.columns[df_unique_seq.apply(lambda x: x.apply(lambda y: (hasattr(y,'__len__') and len(y) > 5 ) ), axis=1).any()]\n",
    "else:\n",
    "    # Eliminate spatio-temporal information\n",
    "#     df_unique_drop_columns = df_unique_seq.columns[df_unique_seq.apply(lambda x: x.apply(lambda y: (isinstance(y,float) and np.isnan(y)) or (len(y) > 0 and isinstance(y[0],bytes)) or ( len(y) >= np.min([*x['Cardiac Number of Images'],len(x['Slice Location'])]) )  ), axis=1).any()]\n",
    "    df_unique_too_long_columns = df_unique_seq.columns[df_unique_seq.apply(lambda x: x.apply(lambda y: (hasattr(y,'__len__') and len(y) >= np.min([*x['Cardiac Number of Images'],len(x['Slice Location'])]) )  ), axis=1).any()]\n",
    "print(\"Dropping too long (spatio-temporal information in fl3d1) columns: {}\".format(df_unique_too_long_columns))\n",
    "df_unique_seq_sanitized = df_unique_seq.drop(df_unique_too_long_columns, axis=1)\n",
    "\n",
    "# Choose sequence to visualize\n",
    "df_unique_all_nan_empty_bytes_columns = df_unique_seq_sanitized.columns[df_unique_seq_sanitized.applymap(lambda x: (isinstance(x,float) and np.isnan(x)) or (hasattr(x,'__len__') and len(x) == 0) or (len(x) > 0 and isinstance(x[0],bytes)) ).all()]\n",
    "print(\"Dropping all-NaN/empty/bytes columns: {}\".format(df_unique_all_nan_empty_bytes_columns))\n",
    "df_unique_plot = df_unique_seq_sanitized.drop(df_unique_all_nan_empty_bytes_columns, axis=1)\n",
    "# df_unique_plot = df_unique_seq_sanitized.drop(df_unique_seq_sanitized.columns[df_unique_seq_sanitized.applymap(lambda x: len(x) == 0).all()], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform visualization (incl. some final type conversions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Show non-unique tags\n",
    "print(\"Columns with multiple values (treated as stacked): {}.\".format(df_unique_plot.columns[df_unique_plot.applymap(lambda x: hasattr(x,'__len__') and len(x) > 1).any()]))\n",
    "\n",
    "# # df_unique_plot_repr = df_unique_plot.apply(lambda x: x.apply(lambda y: y[0]) if x.apply(lambda y: len(y) == 1).all() else x.apply(lambda y: tuple(y)) )\n",
    "# tuple_with_newlines = lambda x : x if not isinstance(x,tuple) else '(' + ',\\n'.join([str(el) for el in x]) + ')'\n",
    "# df_unique_plot_repr = df_unique_plot.apply(lambda x: x.apply(lambda y: tuple_with_newlines(y[0])) if x.apply(lambda y: len(y) == 1).all() else x.apply(lambda y: tuple([tuple_with_newlines(el) for el in y])) )\n",
    "\n",
    "# num_columns = len(df_unique_plot_repr.columns)\n",
    "num_columns = len(df_unique_plot.columns)\n",
    "ncols=4\n",
    "nrows=(num_columns+ncols-1)//ncols\n",
    "fig, ax = plt.subplots(nrows=nrows , ncols=ncols, figsize=(12*ncols,20*nrows))\n",
    "axs = ax.flatten()\n",
    "\n",
    "# for i, col in enumerate(df_unique_plot_repr.columns[:num_columns]):\n",
    "for i, col in enumerate(df_unique_plot.columns[:num_columns]):\n",
    "    \n",
    "    # Filter any left over NaNs\n",
    "    df_unique_plot_is_not_nan = df_unique_plot[col].apply(lambda y: not( (isinstance(y,float) and np.isnan(y)) ) )\n",
    "    df_unique_plot_col = df_unique_plot[col][df_unique_plot_is_not_nan]\n",
    "    \n",
    "    def get_tuple_type(el):\n",
    "        if not isinstance(el,(tuple,list,np.ndarray)):\n",
    "            return type(el)\n",
    "        else:\n",
    "            return tuple([get_tuple_type(el_i) for el_i in el])\n",
    "            \n",
    "#     value_types = df_unique_plot_repr[col].apply(type).values\n",
    "    value_types = set()\n",
    "    for t in df_unique_plot_col.apply(lambda x: get_tuple_type(x) ).unique(): # unique types across series\n",
    "        # print(\"Value type: {}\".format(t), flush=True)\n",
    "        for t_i in t: # unique types within series\n",
    "            value_types.add(t_i)\n",
    "    print(\"Plotting {}\".format(col))\n",
    "    print(\"    value types: {}\".format(value_types), flush=True)\n",
    "    if len(value_types) > 1:\n",
    "        print(\"WARNING: column {} with heterogeneous value types: {}\".format(col, value_types), flush=True)\n",
    "\n",
    "    def get_plot_value(col, series_value, value_types): # TODO: For UID values get only the prefix - like '.'.join(uid.split('.')[:-1]) ############################\n",
    "        if isinstance(series_value,tuple):\n",
    "            return  '(' + ',\\n'.join([str(el) for el in series_value]) + ')'\n",
    "        elif col == 'Patient\\'s Age':\n",
    "            assert isinstance(series_value, str)\n",
    "            return int( re.search(r'^(?P<age>\\d{3})Y$', series_value).group('age') )\n",
    "        elif col == 'Patient\\'s Sex':\n",
    "            return series_value if series_value != 'W' else 'F'\n",
    "        elif isinstance(series_value, datetime.date):\n",
    "            if len(value_types) == 1:\n",
    "                return mdates.date2num(series_value)\n",
    "            else: # str is fallback representation\n",
    "                return str(series_value)                \n",
    "        elif isinstance(series_value, datetime.time):\n",
    "            if len(value_types) == 1:\n",
    "                return mdates.date2num(datetime.datetime(1900,1,1,\n",
    "                                                     series_value.hour, series_value.minute, series_value.second, \n",
    "                                                     series_value.microsecond, series_value.tzinfo))\n",
    "            else: # str is fallback representation\n",
    "                return str(series_value)                \n",
    "        elif isinstance(series_value, pydicom.valuerep.PersonName): # or col == 'Rescale Type':\n",
    "            return str(series_value)\n",
    "        elif isinstance(series_value, str) or np.isscalar(series_value):\n",
    "            return series_value\n",
    "        else:\n",
    "            raise ValueError(\"No treatment for {} of type {}\".format(series_value, type(series_value)))\n",
    "\n",
    "    # Distinguish all single-valued from multi-valued columns\n",
    "    if df_unique_plot_col.apply(lambda y: len(y) == 1).all():\n",
    "        df_unique_plot_col_finished = df_unique_plot_col.apply(lambda x: get_plot_value(col, x[0], value_types)) # newlines are rather for tick labels\n",
    "    else:\n",
    "        df_unique_plot_col_finished = df_unique_plot_col.apply(lambda x: tuple([ get_plot_value(col, x_i, value_types) for x_i in x])) # newlines are rather for tick labels\n",
    "        if df_unique_plot_col_finished.apply(lambda y: len(y) == 0).any(): # e.g. needed if col == 'Rescale Type':\n",
    "            df_unique_plot_col_finished = df_unique_plot_col_finished.apply(repr)\n",
    "\n",
    "    axs[i].hist(df_unique_plot_col_finished.values, label=col, bins=30, stacked=True)\n",
    "    \n",
    "    if datetime.date in value_types and len(value_types) == 1:\n",
    "#         axs[i].xaxis.set_major_locator(mdates.YearLocator())\n",
    "#         axs[i].xaxis.set_major_formatter(mdates.DateFormatter('%d.%m.%y'))\n",
    "        locator = mdates.AutoDateLocator()\n",
    "        axs[i].xaxis.set_major_locator(locator)\n",
    "        axs[i].xaxis.set_major_formatter(mdates.AutoDateFormatter(locator))\n",
    "    elif datetime.time in value_types and len(value_types) == 1:\n",
    "        axs[i].xaxis.set_major_locator(mdates.HourLocator())\n",
    "        axs[i].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "        \n",
    "    \n",
    "#     if col != 'Rescale Type':\n",
    "#         axs[i].hist(df_unique_plot_col_finished.values, label=col, bins=30, stacked=True)\n",
    "#     else:\n",
    "#         axs[i].hist(df_unique_plot_col_finished.apply(repr).values, label=col, bins=30, stacked=True)\n",
    "        \n",
    "\n",
    "    \n",
    "#     if len(value_types) == 1:\n",
    "#         if isinstance(list(value_types)[0],tuple): # maybe this can also handle situations with multiple values per series, each being a tuple\n",
    "#             axs[i].hist(df_unique_plot_col.apply(lambda y: '(' + ',\\n'.join([str(el) for el in y[0]]) + ')').values , label=col, bins=30, stacked=True)            \n",
    "#         elif not(datetime.time in value_types or datetime.date in value_types or pydicom.valuerep.PersonName in value_types or col == 'Rescale Type' ):\n",
    "#             axs[i].hist(df_unique_plot_col.values, label=col, bins=30, stacked=True)\n",
    "#         else:\n",
    "#             axs[i].hist(df_unique_plot_col.apply(str).values, label=col, bins=30, stacked=True)\n",
    "#     else:\n",
    "#         print(\"Column {} with heterogeneous value types: {}\".format(col, value_types), flush=True)\n",
    "#         axs[i].hist(df_unique_plot_col.values, label=col, bins=30, stacked=True)\n",
    "        \n",
    "\n",
    "#     if not(datetime.time in value_types or datetime.date in value_types or pydicom.valuerep.PersonName in value_types or col == 'Rescale Type' ):\n",
    "#         axs[i].hist(df_unique_plot_repr[col].values, label=col, bins=30, stacked=True)\n",
    "#     else:\n",
    "#         axs[i].hist(df_unique_plot_repr[col].apply(str).values, label=col, bins=30, stacked=True)\n",
    "# #         axs[i].hist(df_unique_plot_repr.apply(lambda x: datetime.datetime(x[col[:-4]+\"Date\"].year, x[col[:-4]+\"Date\"].month, x[col[:-4]+\"Date\"].day, x[col].hour, x[col].minute, x[col].second, x[col].microsecond), axis=1).values, label=col, bins=30, stacked=True)        \n",
    "\n",
    "    title_num_nans = (~df_unique_plot_is_not_nan).sum()\n",
    "    title_num_rows = df_unique_plot_is_not_nan.shape[0]\n",
    "    title_addendum = \" ({}/{} NaNs excluded)\".format(title_num_nans, title_num_rows) if title_num_nans != 0 else \"\"\n",
    "    axs[i].set_title(col + title_addendum) # could also add value_types if desired\n",
    "    axs[i].tick_params(labelrotation=90)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
