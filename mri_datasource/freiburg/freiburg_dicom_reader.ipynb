{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICOM FlowMRI Reader - analysis of DICOM files and conversion to hpc-predict-io HDF5-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Notebook parameters - to be moved into papermill parameters\n",
    "import os\n",
    "hpc_predict_code_root = \"/src/hpc-predict\"\n",
    "hpc_predict_data_root = \"/hpc-predict-data\" # os.path.join(os.environ['HOME'], \"src/hpc-predict/hpc-predict/data/v1\")\n",
    "\n",
    "mri_samples = [3,]\n",
    "tar_files = True\n",
    "mri_data_root = \"input_data/original/mri/MRT Daten Bern tar\"\n",
    "output_root   = \"input_data/preprocessed/mri/MRT Daten Bern tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Turn mri_samples into a list if it isn't one yet (papermill)\n",
    "if type(mri_samples) is not list:\n",
    "    mri_samples = [mri_samples]\n",
    "    \n",
    "# Bash version of list\n",
    "mri_samples_bash = ' '.join([str(sample) for sample in mri_samples])\n",
    "tar_files_bash = '--tar' if tar_files else ''\n",
    "mri_samples_bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env hpc_predict_code_root = {hpc_predict_code_root}\n",
    "%env hpc_predict_data_root = {hpc_predict_data_root}\n",
    "%env mri_data_root = {mri_data_root}\n",
    "%env mri_samples = {mri_samples_bash}\n",
    "%env tar_files = {tar_files_bash}\n",
    "%env output_root = {output_root}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICOM header extraction into Pandas DataFrame and overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script assumes that you have the Freiburg dataset available, otherwise first run `data/fetch_scripts/fetch_freiburg_to_tar.sh data/tmp data/v1`.\n",
    "\n",
    "To run the following steps, the DICOM headers must previously be converted to pandas tables, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -x\n",
    "python3  \"${hpc_predict_code_root}\"/hpc-predict-io/mri_datasource/freiburg/convert_dicom_headers_to_pandas_df.py --hpc-predict-data-root \"${hpc_predict_data_root}\" --mri-data-root \"${mri_data_root}\" --mri-samples ${mri_samples} ${tar_files} --output-root \"${output_root}\"\n",
    "set +x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tarfile\n",
    "import json\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from mr_io import FlowMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Possibly duplicate with code in convert_dicom_headers_to_pandas_df\n",
    "\n",
    "if os.path.exists(os.path.join(hpc_predict_data_root, 'encrypt')):\n",
    "    hpc_predict_data_root = os.path.join(hpc_predict_data_root, 'decrypt')\n",
    "    assert os.path.exists(hpc_predict_data_root)\n",
    "\n",
    "mri_data_root = os.path.join(hpc_predict_data_root, mri_data_root)\n",
    "assert os.path.exists(mri_data_root)\n",
    "\n",
    "for sample in mri_samples:\n",
    "    assert os.path.exists(os.path.join(mri_data_root, str(sample) + ('.tar' if tar_files else '/') ))\n",
    "\n",
    "if tar_files:\n",
    "    mri_sample_tar_paths = [os.path.relpath(os.path.join(mri_data_root, str(sample) + '.tar'), start=hpc_predict_data_root) for sample in mri_samples]\n",
    "    \n",
    "\n",
    "output_root = os.path.join(hpc_predict_data_root, output_root)\n",
    "assert os.path.exists(output_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load pickled DataFrames and metainformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(hpc_predict_data_root, mri_data_root,\"venc.json\"), 'r') as f:\n",
    "    venc = json.load(f)\n",
    "venc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pkls = sorted([os.path.join(output_root, str(sample), \"dicom_header_df.pkl\") for sample in mri_samples], key=lambda x: int(os.path.basename(os.path.dirname(x))) ) # os.path.basename(x).split('.')[0]\n",
    "pkls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pandas DataFrame that contains pydicom DataElement objects in individual entries \n",
    "df = pd.concat([pd.read_pickle(pkl) for pkl in pkls],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# A pydicom Dataelement: The value is held in the _value attribute, \n",
    "# often in \"semi-serialized\" representation with the VR describing the DICOM type  \n",
    "vars(df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Use names instead of DICOM tags as column labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Compute the DICOM tag -> name mapping to rename columns\n",
    "tag_names = df.apply( lambda c: c.dropna().apply(lambda x: x.name).unique(), axis=0); assert tag_names.shape[0] == 1; tag_names = tag_names.loc[0] \n",
    "#tag_names.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = df.rename(columns=tag_names.to_dict())\n",
    "\n",
    "# display only values\n",
    "pd.set_option('display.max_columns', 20)\n",
    "df_renamed.applymap(lambda x: x.value if pd.notnull(x) else x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Determine range of DICOM tags (via unique rows, grouped by patient ID > study UID > series UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine types of DataElement.value entries\n",
    "from collections import namedtuple, OrderedDict\n",
    "from types import SimpleNamespace\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.concat([pd.Series(name=name, data=col.dropna().unique()) for (name, col) in df_renamed.applymap(lambda x: (x.VR, type(x.value)) if pd.notnull(x) else x).iteritems()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract native Python datatypes (built-in/standard library) from pydicom DataElements\n",
    "def get_native_from_pydicom(val):\n",
    "    from pydicom.valuerep import DA, DT, TM, DSfloat, DSdecimal, IS, PersonName\n",
    "    from pydicom.uid import UID\n",
    "    from pydicom.multival import MultiValue\n",
    "    from datetime import (date, datetime, time, timedelta, timezone)\n",
    "    from decimal import Decimal\n",
    "\n",
    "    if isinstance(val, DA):\n",
    "        return date(val.year, val.month, val.day)\n",
    "    elif isinstance(val, DT):\n",
    "        return datetime(val.year, val.month, val.day,\n",
    "                        val.hour, val.minute, val.second,\n",
    "                        val.microsecond, val.tzinfo)\n",
    "    elif isinstance(val, TM):\n",
    "        return time(val.hour, val.minute, val.second,\n",
    "                    val.microsecond)\n",
    "    elif isinstance(val, DSfloat):\n",
    "        return float(val)\n",
    "    elif isinstance(val, DSdecimal):\n",
    "        return Decimal(val)\n",
    "    elif isinstance(val, IS):\n",
    "        return int(val)\n",
    "    elif isinstance(val, MultiValue):\n",
    "        return tuple([get_native_from_pydicom(el) for el in val])\n",
    "    elif isinstance(val, UID):\n",
    "        return str(val)\n",
    "    elif isinstance(val, (PersonName, str, int, float)):\n",
    "        return val\n",
    "    elif isinstance(val, list):\n",
    "        return tuple(val)\n",
    "    else:\n",
    "        return val        \n",
    "\n",
    "def unpack_pydicom_value(value):\n",
    "    if hasattr(type(value),'__module__') and type(value).__module__.startswith('pydicom'):\n",
    "        return get_native_from_pydicom(value)\n",
    "    elif isinstance(value, list):\n",
    "        return tuple([unpack_pydicom_value(v) for v in value])\n",
    "    elif isinstance(value,OrderedDict):\n",
    "        return tuple( [unpack_pydicom_value(v.value) for v in value.values()] )\n",
    "    elif isinstance(value, dict):\n",
    "        return tuple(value.items())\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "def agg_unique(group):\n",
    "    labels = []\n",
    "    row = []\n",
    "    for (name, col) in group[group.columns[:]].iteritems():\n",
    "        labels.append(name)\n",
    "        row.append(col.dropna().unique())\n",
    "\n",
    "    return pd.DataFrame([row], columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_native_renamed = df_renamed.applymap(lambda x: unpack_pydicom_value(x.value) if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-by patient/study/series ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_series_keys = ['FileCollectionID',\n",
    "                        'Instance Creation Date',\n",
    "                        'Patient ID',\n",
    "                        'Study Instance UID',\n",
    "                        'Sequence Name',\n",
    "                        'Series Instance UID',\n",
    "                        'Image Type']\n",
    "\n",
    "df_native_grouped = df_native_renamed.groupby(group_by_series_keys)\n",
    "df_native_grouped_unique = df_native_grouped.apply(lambda x: agg_unique(x))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All tags with <= 1 unique values per series in any series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_native_grouped_unique_singleton_cols = df_native_grouped_unique.applymap(lambda x: len(x) <= 1).all()\n",
    "df_native_grouped_unique.drop(\n",
    "    df_native_grouped_unique.columns[~df_native_grouped_unique_singleton_cols], axis=1).applymap(lambda x: x[0] if len(x) == 1 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...and number of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_native_grouped_unique.drop(\n",
    "    df_native_grouped_unique.columns[df_native_grouped_unique_singleton_cols],axis=1).applymap(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All tags with more than a single unique values in at least one series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_native_grouped_unique.drop(\n",
    "    df_native_grouped_unique.columns[df_native_grouped_unique_singleton_cols],axis=1).applymap(\n",
    "    lambda x: x[0] if len(x) == 1 else x).drop(['FilePath'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DICOM header analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Analysis: Referenced Image Sequence tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_referenced_uid = df_native_renamed['Referenced Image Sequence'].iloc[0][0][1]\n",
    "\n",
    "# Check if we can find the referenced image sequence\n",
    "print(\"The string length of a Referenced Image Sequence UID is {}: {}.\".format(len(example_referenced_uid), example_referenced_uid))\n",
    "        \n",
    "def get_uid_prefix(uid):\n",
    "    return '.'.join(uid.split('.')[:-1])\n",
    "example_referenced_uid_prefix = get_uid_prefix(example_referenced_uid)\n",
    "print(\"Example referenced UID prefix is {}\".format(example_referenced_uid_prefix))\n",
    "\n",
    "print(\"Share of SOP Instance UIDs with same prefix up to and after last point: {}.\".format(', '.join([\n",
    "    str(100.*\n",
    "    (df_native_renamed['SOP Instance UID'].apply(lambda x: x[:prefix_len] ) == example_referenced_uid[:prefix_len] ).sum()/\n",
    "    df_native_renamed.shape[0]) + \" %\" for prefix_len in range(len(example_referenced_uid_prefix),len(example_referenced_uid_prefix)+10)]) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Conclusion: Referenced Image Sequence Tag identical before last point, afterwards ~random - the prefix 1.2.40.0.13.1 is taken from the dcm4che DICOM implementation, the remainder is a randomly generated UID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of time (Acquisition time, Content time, Trigger time and Nominal interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime  \n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_series_title = \"Group by series UID plot (\" + \\\n",
    "                        \" / \".join([group_by_series_keys[0],\n",
    "                                    group_by_series_keys[1],\n",
    "                                    group_by_series_keys[4],\n",
    "                                    group_by_series_keys[6],\n",
    "                                    'Nominal Interval',\n",
    "                                    'Cardiac Number of Images',\n",
    "                                    '[P-index]'\n",
    "                                  ]) + \\\n",
    "                        \")\"\n",
    "\n",
    "def legend_unique_values(ls):\n",
    "    return str(ls[0]) if len(ls)==1 else \"{}..{}\".format(ls.min(), ls.max()) \n",
    "\n",
    "def group_by_series_legend(name, group, j=None):\n",
    "    nominal_intervals = group['Nominal Interval'].unique() #.apply(lambda x: x.original_string).unique()\n",
    "    cardiac_number_of_images = group['Cardiac Number of Images'].unique() #.apply(lambda x: x.original_string).unique()\n",
    "    return \", \".join([\n",
    "            str(name[0]),\n",
    "            str(name[1]),\n",
    "            name[4],\n",
    "            str(name[6]), \n",
    "            \"NomInt {} ms\".format(legend_unique_values(nominal_intervals)),\n",
    "            \"CardImgs {}\".format(legend_unique_values(cardiac_number_of_images))\n",
    "            ]) + (\", P-{}\".format(j) if j is not None else \"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#from inspect import signature\n",
    "\n",
    "# groups = df.\\\n",
    "#     applymap(lambda x: unpack_pydicom_value(x.value) if pd.notnull(x) else x).\\\n",
    "#     groupby(group_by_series_keys)\n",
    "\n",
    "def make_group_by_plot(groups, x_series, x_label, y_series, y_label):    \n",
    "    \n",
    "    ngroups = len(groups.groups)\n",
    "    ncols = 3\n",
    "    nrows = (ngroups+3-1)//ncols    \n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=nrows,ncols=ncols, sharex=True, figsize=(12*ncols,8*nrows))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i, (name, group) in enumerate(groups): \n",
    "\n",
    "        if 'P' not in group['Image Type'].iloc[0]:\n",
    "            ax[i].plot(x_series(group), \n",
    "                       y_series(group), \n",
    "                       'o',  ms=6, linewidth=1, \n",
    "                       label=group_by_series_legend(name,group))\n",
    "        else:        \n",
    "            for j in range(3):\n",
    "                select_subgroup = group['Instance Number'].apply(lambda x: j*group.shape[0]//3 < x and x <= (j+1)*group.shape[0]//3)            \n",
    "                ax[i].plot(x_series(group[select_subgroup]), \n",
    "                           y_series(group[select_subgroup]),\n",
    "                           'o',  ms=6, linewidth=1, \n",
    "                           label=group_by_series_legend(name,group,j))\n",
    "\n",
    "    for a in ax[:ngroups]: \n",
    "        a.set_xlabel(x_label)\n",
    "        a.set_ylabel(y_label)\n",
    "\n",
    "        a.legend(bbox_to_anchor=(-0.3, 1.04), loc=\"lower left\")\n",
    "        a.grid()\n",
    "    fig.suptitle(group_by_series_title)\n",
    "    fig.tight_layout(pad=3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquisition Time vs. Trigger Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquisition_datetime(df):\n",
    "    return df.apply(\n",
    "        lambda x: datetime.datetime(x['Series Date'].year, \n",
    "                                    x['Series Date'].month, \n",
    "                                    x['Series Date'].day, \n",
    "                                    x['Acquisition Time'].hour, \n",
    "                                    x['Acquisition Time'].minute,\n",
    "                                    x['Acquisition Time'].second, \n",
    "                                    x['Acquisition Time'].microsecond, \n",
    "                                    x['Acquisition Time'].tzinfo ), axis=1)\n",
    "\n",
    "def acqusition_datetime_offset(df):\n",
    "    acquisition_datetime_series = acquisition_datetime(df)\n",
    "    return acquisition_datetime_series - acquisition_datetime_series.min()\n",
    "\n",
    "\n",
    "group_x_series = lambda group: group['Trigger Time']\n",
    "x_label = 'Trigger Time [ms]'\n",
    "group_y_series = lambda group: acqusition_datetime_offset(group).apply(lambda x: x.total_seconds()*1e3)+group['Trigger Time'].min()\n",
    "y_label = 'Acquisition Time (offset to min) [ms]'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the Trigger Time and Acquisition time coincide up to an offset for Acquisition time. Also, they do not cover the entire heart cycle but only a bit more than the first half. This could be the case since it may be a prospective hear MRI as explained [here](http://mriquestions.com/gating-parameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Trigger time vs. nominal interval\n",
    "pd.concat([df_native_grouped_unique[['Trigger Time']].applymap(lambda x: np.max(x)), df_native_grouped_unique[['Nominal Interval']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Content Time vs. Trigger Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def content_datetime(df):\n",
    "    return df.apply(\n",
    "        lambda x: datetime.datetime(x['Series Date'].year, \n",
    "                                    x['Series Date'].month, \n",
    "                                    x['Series Date'].day, \n",
    "                                    x['Content Time'].hour, \n",
    "                                    x['Content Time'].minute,\n",
    "                                    x['Content Time'].second, \n",
    "                                    x['Content Time'].microsecond, \n",
    "                                    x['Content Time'].tzinfo ), axis=1)\n",
    "\n",
    "def content_datetime_offset(df):\n",
    "    content_datetime_series = content_datetime(df)\n",
    "    return content_datetime_series - content_datetime_series.min()\n",
    "\n",
    "group_x_series = lambda group: group['Trigger Time']\n",
    "x_label = 'Trigger Time [ms]'\n",
    "group_y_series = lambda group: content_datetime_offset(group).apply(lambda x: x.total_seconds()*1e3)+group['Trigger Time'].min()\n",
    "y_label = 'Content Time (offset to min) [ms]'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that the Content Time refers to the time when the MRI signal was recorded - the entire sequence is spread over more than 40 seconds. No structure apparent from these plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Trigger Time vs. Instance Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "group_x_series = lambda group: group['Instance Number'] \n",
    "x_label = 'Instance Number'\n",
    "group_y_series = lambda group:  group['Trigger Time'] \n",
    "y_label = 'Trigger Time [ms]'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These plots clearly suggest that the P-series contain 3 sub-series that are sorted according to instance number (the visible plateau is due to multiple slices per fixed time being recorded one after another). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Content Time vs. Instance Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "group_x_series = lambda group: group['Instance Number']\n",
    "x_label = 'Instance Number'\n",
    "group_y_series = lambda group: content_datetime_offset(group).apply(lambda x: x.total_seconds()*1e3)+group['Trigger Time'].min()\n",
    "y_label = 'Content Time (offset to min) [ms]'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No correlation between content time and instance number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Instance Creation Time vs. Instance Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def instance_creation_datetime(df):\n",
    "    return df.apply(\n",
    "        lambda x: datetime.datetime(x['Series Date'].year, \n",
    "                                    x['Series Date'].month, \n",
    "                                    x['Series Date'].day, \n",
    "                                    x['Instance Creation Time'].hour, \n",
    "                                    x['Instance Creation Time'].minute,\n",
    "                                    x['Instance Creation Time'].second, \n",
    "                                    x['Instance Creation Time'].microsecond, \n",
    "                                    x['Instance Creation Time'].tzinfo ), axis=1)\n",
    "\n",
    "def instance_creation_datetime_offset(df):\n",
    "    content_datetime_series = content_datetime(df)\n",
    "    return content_datetime_series - content_datetime_series.min()\n",
    "\n",
    "group_x_series = lambda group: group['Instance Number']\n",
    "x_label = 'Instance Number'\n",
    "group_y_series = lambda group: instance_creation_datetime_offset(group).apply(lambda x: x.total_seconds()*1e3)+group['Trigger Time'].min()\n",
    "y_label = 'Instance Creation Time (offset to min) [ms]'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SOP Instance UID vs. Instance Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "group_x_series = lambda group: group['Instance Number'] \n",
    "x_label = 'Instance Number'\n",
    "group_y_series = lambda group:  group['SOP Instance UID'].apply(lambda x: int(x.split('.')[-1]))\n",
    "y_label = 'SOP Instance UID'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Unique SOP Instance UID prefixes\n",
    "df_native_renamed['SOP Instance UID'].apply(lambda x: get_uid_prefix(x)).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The SOP Instance UID seems to be an Implementation label for dcm4che followed by a randomly generated UID with no correlation with Instance Number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice Location vs. Instance Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_x_series = lambda group: group['Instance Number']\n",
    "x_label = 'Instance Number'\n",
    "group_y_series = lambda group: group['Slice Location']\n",
    "y_label = 'Slice Location'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice Location (col x row coord sys) - Slice Location (DICOM) vs. Instance Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "group_x_series = lambda group: group['Instance Number'] #.apply(lambda x: float(x.original_string))\n",
    "x_label = 'Instance Number'\n",
    "\n",
    "def slice_location_image_coord_sys(group):\n",
    "    seq_origin = np.array(group.loc[group['Instance Number'].idxmin(),'Image Position (Patient)'])    \n",
    "    \n",
    "    orientation = group['Image Orientation (Patient)'].unique()\n",
    "    assert len(orientation) == 1\n",
    "\n",
    "    row_unit = np.array(orientation[0][:3])\n",
    "    col_unit = np.array(orientation[0][3:])\n",
    "    ort_unit = np.cross(col_unit, row_unit)\n",
    "\n",
    "    return group['Image Position (Patient)'].apply(lambda x: np.dot(ort_unit, np.array(x)-seq_origin) )\n",
    "\n",
    "def slice_location_diff(group):\n",
    "    return slice_location_image_coord_sys(group) - group['Slice Location'].apply(lambda x: np.array(x))\n",
    "\n",
    "group_y_series = slice_location_diff\n",
    "y_label = 'Slice Location (col x row) - Slice Location (DICOM)'\n",
    "\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The coordinate system of the DICOM images produces the same alignment (with \"Slice Location\" as z-axis) when taking the column direction as the primary and row-direction as the secondary axis (i.e. x runs along the columns, y along the rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check orthogonality of image planes and relative image offset vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (name, group) in enumerate(pd.concat([df_renamed, instance_creation_datetime], axis=1).\\\n",
    "#     applymap(lambda x: unpack_pydicom_value(x.value) if pd.notnull(x) else x).\\\n",
    "#     groupby(['FileCollectionID', 'Instance Creation Date', 'Patient ID', 'Study Instance UID', 'Series Instance UID', 'Image Type', 'Sequence Name'])): # group['Instance Creation Date'].iloc[0]\n",
    "\n",
    "def print_if_above_numeric_tol(name, value, tol=1e-11):\n",
    "    if value > tol:\n",
    "        print(\"    FAIL: {} = {} exceeds numeric tolerance \".format(name, value))\n",
    "        raise RuntimeError()\n",
    "    else:\n",
    "        print(\"    OK:   {} = {} \".format(name, value))\n",
    "            \n",
    "\n",
    "for i, (name, group) in enumerate(df_native_grouped):\n",
    "\n",
    "    orientation = group['Image Orientation (Patient)'].unique()\n",
    "    assert len(orientation) == 1\n",
    "\n",
    "    row_unit = np.array(orientation[0][:3])\n",
    "    col_unit = np.array(orientation[0][3:])\n",
    "    ort_unit = np.cross(col_unit, row_unit)\n",
    "    \n",
    "\n",
    "    if 'P' not in group['Image Type'].iloc[0]:\n",
    "\n",
    "        print(name[0],name[1],name[4], name[6])\n",
    "\n",
    "        positions = group.sort_values('Instance Number')['Image Position (Patient)'].apply(lambda x: np.array(x))    \n",
    "        #pprint((positions.iloc[1:] - positions[:-1]).apply(lambda x: np.linalg.norm(x)).max())\n",
    "        if len(positions) > 1:\n",
    "            print_if_above_numeric_tol(\"max(dot(col_unit, patient_position_offset))\", np.abs(np.dot( np.vstack(positions.to_numpy()[1:] - positions.to_numpy()[:-1]), col_unit) ).max() )\n",
    "            print_if_above_numeric_tol(\"max(dot(row_unit, patient_position_offset))\", np.abs(np.dot( np.vstack(positions.to_numpy()[1:] - positions.to_numpy()[:-1]), row_unit) ).max() )\n",
    "\n",
    "    else:\n",
    "        for j in range(3):\n",
    "\n",
    "            print(name[0], name[1], name[4], name[6], j)\n",
    "\n",
    "            select_subgroup = group['Instance Number'].apply(lambda x: j*group.shape[0]//3 < x and x <= (j+1)*group.shape[0]//3)            \n",
    "\n",
    "            positions = group[select_subgroup].sort_values('Instance Number')['Image Position (Patient)'].apply(lambda x: np.array(x))\n",
    "            if len(positions) > 1:\n",
    "                print_if_above_numeric_tol(\"max(dot(col_unit, patient_position_offset))\", np.abs(np.dot( np.vstack(positions.to_numpy()[1:] - positions.to_numpy()[:-1]), col_unit) ).max() )\n",
    "                print_if_above_numeric_tol(\"max(dot(row_unit, patient_position_offset))\", np.abs(np.dot( np.vstack(positions.to_numpy()[1:] - positions.to_numpy()[:-1]), row_unit) ).max() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Image planes are orthogonal to Image Position (Patient) offset vector (note that the calculation wraps around time slice edges, i.e. it computes offsets between last and first Image Position (Patient), which, however, does not affect the conclusion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# - Check that Cardiac number of images coincides with number of times (trigger/acquisition)\n",
    "# - number of slice locations coincides with number of image position patients\n",
    "# - and that both together with Rows and Columns explain the number of instances\n",
    "# - statistics over unique values per series and conversion to HPC-PREDICT-IO/create MRI writer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conversion to hpc-predict-io HDF5-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_native_grouped_unique[['Image Type', 'FilePath_4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Found sequence names {} (only processing fl3d1).\".format(df_native_renamed['Sequence Name'].unique()))\n",
    "\n",
    "group_by_series_keys = ['FileCollectionID', 'Instance Creation Date', 'Patient ID', 'Study Instance UID', 'Sequence Name'] #, 'Series Instance UID', 'Image Type']\n",
    "df_flash_grouped = df_native_renamed[df_native_renamed['Sequence Name'] == 'fl3d1'].groupby(group_by_series_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tar_files:\n",
    "    tar_files_dict = {\n",
    "        tar_path: tarfile.open(os.path.join(hpc_predict_data_root, tar_path)) \n",
    "        for tar_path in mri_sample_tar_paths\n",
    "    }\n",
    "    \n",
    "def dcmread(file_path):\n",
    "    if not tar_files:\n",
    "        return pydicom.dcmread(os.path.join(hpc_predict_data_root, file_path))\n",
    "    else:\n",
    "#         tar = tarfile.open(os.path.join(hpc_predict_data_root, file_path.split(':')[0]))\n",
    "        tar = tar_files_dict[file_path.split(':')[0]]\n",
    "        dcmfile = tar.extractfile(file_path.split(':')[1])\n",
    "        return pydicom.filereader.dcmread(dcmfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sequence handler for fl3d1, that for Nav can be null\n",
    "\n",
    "flow_mris = []\n",
    "\n",
    "def extract_unique_value(group, col):\n",
    "    value = group[col].unique()\n",
    "    assert len(value) == 1; \n",
    "    return value[0]\n",
    "\n",
    "for (name, group) in df_flash_grouped:\n",
    "    print(name,flush=True)\n",
    "    \n",
    "    file_path = dict(M=set(), P=set(), M_files=[], P_files=[])\n",
    "    magnitude = None\n",
    "    phase = []\n",
    "    \n",
    "    for (series_name, series_group) in group.groupby(['Series Instance UID', 'Image Type']):\n",
    "        if 'M' in series_name[1]:\n",
    "            magnitude = series_group.sort_values('Instance Number')\n",
    "            file_path['M'].add(extract_unique_value(series_group, 'FilePath_4'))\n",
    "            file_path['M_files'].append(list(magnitude['FilePath'].values))\n",
    "        elif 'P' in series_name[1]:\n",
    "            phase_group = series_group.sort_values('Instance Number')\n",
    "            phase = [ phase_group[ phase_group['Instance Number'].apply(\n",
    "                lambda x: j*phase_group.shape[0]//3 < x and x <= (j+1)*phase_group.shape[0]//3) ] \n",
    "                     for j in range(3) ]\n",
    "            file_path['P'].add(extract_unique_value(series_group, 'FilePath_4'))\n",
    "            file_path['P_files'].append([ list(phase[j]['FilePath'].values) for j in range(3) ])\n",
    "\n",
    "    file_path['M'] = list(file_path['M']) \n",
    "    file_path['P'] = list(file_path['P']) \n",
    "    file_path['root'] = extract_unique_value(group, 'FilePath_3').rsplit(':', maxsplit=1)[-1]\n",
    "\n",
    "    num_rows = extract_unique_value(group, 'Rows')\n",
    "    num_cols = extract_unique_value(group, 'Columns')\n",
    "    num_slice_locations = len(group['Slice Location'].unique()) # FIXME: sanity check!\n",
    "    cardiac_number_of_images = extract_unique_value(group, 'Cardiac Number of Images')\n",
    "    \n",
    "    assert magnitude.shape[0] == num_slice_locations*cardiac_number_of_images\n",
    "    for j in range(3):\n",
    "        assert phase[j].shape[0] == num_slice_locations*cardiac_number_of_images\n",
    "\n",
    "    orientation = extract_unique_value(group, 'Image Orientation (Patient)')\n",
    "    row_unit = np.array(orientation[:3])\n",
    "    col_unit = np.array(orientation[3:])\n",
    "    ort_unit = np.cross(col_unit, row_unit)\n",
    "    assert (np.linalg.norm(ort_unit) - 1.) < 1e-12\n",
    "\n",
    "    magnitude_positions = magnitude['Image Position (Patient)'].apply(lambda x: np.array(x))    \n",
    "    magnitude_position_origin = np.array(magnitude.loc[magnitude['Instance Number'].idxmin(),'Image Position (Patient)'])    \n",
    "    magnitude_slice_position_shift = magnitude_positions.apply(lambda x: np.dot(ort_unit,x - magnitude_position_origin)) - magnitude['Slice Location']    \n",
    "    assert (magnitude_slice_position_shift - magnitude_slice_position_shift.mean()).abs().max() < 1e-11\n",
    "    \n",
    "    slice_locations = magnitude['Slice Location'][:num_slice_locations].values # FIXME: sanity check!\n",
    "    assert len(np.unique(slice_locations)) == num_slice_locations\n",
    "#     magnitude_slice_locations = magnitude['Slice Location'][:num_slice_locations].values    \n",
    "    magnitude_trigger_times = magnitude['Trigger Time'][::num_slice_locations].values\n",
    "    assert len(np.unique(magnitude_trigger_times)) == cardiac_number_of_images\n",
    "    magnitude_values = np.ndarray(shape=(num_rows, num_cols, num_slice_locations, cardiac_number_of_images))\n",
    "    # TODO: Check correctness of pixel value reading\n",
    "    for t,_ in enumerate(magnitude_trigger_times):\n",
    "        for z,_ in enumerate(slice_locations): # magnitude_slice_locations\n",
    "            img_row = magnitude[magnitude['Instance Number'] == t*num_slice_locations + z + 1]; assert len(img_row) == 1; img_row = img_row.iloc[0]\n",
    "            dcm_img = dcmread(img_row['FilePath'])\n",
    "            magnitude_values[:,:,z,t] = dcm_img.pixel_array # no rescale properties\n",
    "   \n",
    "\n",
    "    phase_positions = []\n",
    "    phase_slice_locations = []\n",
    "    phase_trigger_times = []\n",
    "    phase_values = np.ndarray(shape=(num_rows, num_cols, num_slice_locations, cardiac_number_of_images, 3))\n",
    "    for j in range(3):\n",
    "        # If this does not work can validate the phase z-coordinates here as well\n",
    "        assert (slice_locations == phase[j]['Slice Location'][:num_slice_locations].values).all()\n",
    "        \n",
    "        venc_j = venc['venc_xy'] if j < 2 else venc['venc_z']\n",
    "\n",
    "#         phase_slice_locations.append( phase[j]['Slice Location'][:num_slice_locations].values )    \n",
    "        phase_trigger_times.append( phase[j]['Trigger Time'][::num_slice_locations].values )\n",
    "        print(\"Trigger time offsets of phase[{}] relative to magnitude in {} ms\".format(j, np.unique(phase_trigger_times[j]-magnitude_trigger_times)), flush=True)\n",
    "        for t,_ in enumerate(phase_trigger_times[j]):\n",
    "            for z,_ in enumerate(slice_locations): # phase_slice_locations[j]\n",
    "                img_row = phase[j][phase[j]['Instance Number'] == j*cardiac_number_of_images*num_slice_locations + t*num_slice_locations + z + 1]; assert len(img_row) == 1; img_row = img_row.iloc[0]\n",
    "                dcm_img = dcmread(img_row['FilePath'])\n",
    "                phase_values[:,:,z,t,j] = venc_j/np.abs(dcm_img.RescaleIntercept)*(dcm_img.RescaleSlope*dcm_img.pixel_array+dcm_img.RescaleIntercept)\n",
    "\n",
    "    print(\"Inverting y-phases as visually observed to be along wrong direction in samples (e.g. 10).\")\n",
    "    phase_values[:,:,:,:,1] = -phase_values[:,:,:,:,1]\n",
    "    \n",
    "    #check for uniqueness!\n",
    "    pixel_spacing = extract_unique_value(group, 'Pixel Spacing')\n",
    "    geometry = [pixel_spacing[0]*(np.arange(0,num_rows)+0.5),\n",
    "                pixel_spacing[1]*(np.arange(0,num_cols)+0.5),\n",
    "                slice_locations]\n",
    "\n",
    "    #check for uniqueness!\n",
    "    #group['Nominal Interval'].unique()  #.apply(lambda x: agg_unique(x))\n",
    "    heart_cycle_period = group['Nominal Interval'].mean()\n",
    "    \n",
    "    print(\"Writing {} to HDF5...\".format(name))\n",
    "    flow_mri_metainfo = {\n",
    "        \"file_path\": file_path,\n",
    "        \"venc\": venc,\n",
    "    }\n",
    "\n",
    "    flow_mris.append({\n",
    "        \"study_instance_uid\": name[3],\n",
    "        \"sequence_name\": name[4],\n",
    "        \"cardiac_number_of_images\": cardiac_number_of_images,\n",
    "        \"num_slice_locations\": num_slice_locations,\n",
    "        \"heart_cycle_period\": heart_cycle_period,\n",
    "        \"geometry\": geometry,\n",
    "        \"magnitude_trigger_times\": magnitude_trigger_times,\n",
    "        \"magnitude_values\": magnitude_values, #\"phase_slice_locations\": phase_slice_locations,\n",
    "        \"phase_trigger_times\": phase_trigger_times,\n",
    "        \"phase_values\": phase_values,\n",
    "    })\n",
    "    flow_mris[-1].update(flow_mri_metainfo)\n",
    "    \n",
    "    hpc_predict_mri = FlowMRI(geometry=flow_mris[-1]['geometry'], \n",
    "                          time=flow_mris[-1]['magnitude_trigger_times'], # FIXME: phase times!\n",
    "                          time_heart_cycle_period=flow_mris[-1]['heart_cycle_period'], \n",
    "                          intensity=flow_mris[-1]['magnitude_values'], \n",
    "                          velocity_mean=flow_mris[-1]['phase_values'], \n",
    "                          velocity_cov=None) # np.zeros(shape=phase_values.shape+(3,))) # FIXME: waste of memory space\n",
    "    filename_base = os.path.join(output_root, str(sample) , file_path['root'].replace('/','-'))\n",
    "    hpc_predict_mri.write_hdf5(filename_base + '.h5')\n",
    "    with open(filename_base + '.json', 'w') as flow_mri_metainfo_f:\n",
    "        json.dump(flow_mri_metainfo, flow_mri_metainfo_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hpc_predict_mri = FlowMRI.read_hdf5(filename_base + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure, ax = plt.subplots(ncols=1, figsize=(12*1,8*1))\n",
    "ax.hist(flow_mris[-1]['magnitude_values'].flatten(), bins=50, label='magnitude', hatch='*')\n",
    "ax.hist(hpc_predict_mri.intensity.flatten(), bins=50, label='magnitude-hpc-predict', alpha=0.5)\n",
    "figure.legend()\n",
    "figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(ncols=3, sharex=True, figsize=(12*3,8*1))\n",
    "for j in range(3):\n",
    "    ax[j].hist(flow_mris[0]['phase_values'][:,:,:,:,j].flatten(), bins=50, label='phase-{}'.format(j), hatch='*')\n",
    "    ax[j].hist(hpc_predict_mri.velocity_mean[:,:,:,:,j].flatten(), bins=50, label='phase-{}-hpc-predict'.format(j), alpha=0.5)\n",
    "figure.legend()\n",
    "figure.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
