{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis with Pandas - small number of studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpc_predict_data_dir=\"/home/lukasd/src/hpc-predict/data/v0\" # e.g. ../../../data/v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script assumes that you have the Freiburg dataset available, otherwise first run `data/fetch_scripts/fetch_freiburg.sh`.\n",
    "\n",
    "To run the following steps, the DICOM headers must previously be converted to pandas tables, i.e.\n",
    "\n",
    "```\n",
    "convert_dicom_to_pandas.py --mri-data-root \"${hpc_predict_data_dir}/input_data/original/mri/MRT Daten Bern\" --mri-samples ... --output-root \"${hpc_predict_data_dir}/input_data/original/mri/MRT Daten Bern DICOM Header\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pickled DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "pkls = sorted(glob(hpc_predict_data_dir + \"/input_data/preprocessed/mri/MRT Daten Bern DICOM Header/3.pkl\"), key=lambda x: int(os.path.basename(x).split('.')[0]) )\n",
    "pkls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Read pandas DataFrame that contains pydicom DataElement entries and was generated with convert_dicom_to_pandas.py\n",
    "df = pd.concat([pd.read_pickle(pkl) for pkl in pkls],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual entries are pydicom DataElements\n",
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The value is held in the _value attribute, often in \"semi-serialized\" representation with \n",
    "# the VR describing the DICOM type  \n",
    "vars(df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use names instead of DICOM tags as column labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Compute the DICOM tag -> name mapping to renamed columns\n",
    "tag_names = df.apply( lambda c: c.dropna().apply(lambda x: x.name).unique(), axis=0); assert tag_names.shape[0] == 1; tag_names = tag_names.loc[0] \n",
    "#tag_names.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = df.rename(columns=tag_names.to_dict())\n",
    "\n",
    "# display only values\n",
    "pd.set_option('display.max_columns', 20)\n",
    "df_renamed.applymap(lambda x: x.value if pd.notnull(x) else x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine range of DICOM tags (via unique rows, grouped by patient ID > study UID > series UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine types of DataElement.value entries\n",
    "from collections import namedtuple, OrderedDict\n",
    "from types import SimpleNamespace\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.concat([pd.Series(name=name, data=col.dropna().unique()) for (name, col) in df_renamed.applymap(lambda x: (x.VR, type(x.value)) if pd.notnull(x) else x).iteritems()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract native Python datatypes (built-in/standard library) from pydicom DataElements\n",
    "def get_native_from_pydicom(val):\n",
    "    from pydicom.valuerep import DA, DT, TM, DSfloat, DSdecimal, IS, PersonName\n",
    "    from pydicom.uid import UID\n",
    "    from pydicom.multival import MultiValue\n",
    "    from datetime import (date, datetime, time, timedelta, timezone)\n",
    "    from decimal import Decimal\n",
    "\n",
    "    if isinstance(val, DA):\n",
    "        return date(val.year, val.month, val.day)\n",
    "    elif isinstance(val, DT):\n",
    "        return datetime(val.year, val.month, val.day,\n",
    "                        val.hour, val.minute, val.second,\n",
    "                        val.microsecond, val.tzinfo)\n",
    "    elif isinstance(val, TM):\n",
    "        return time(val.hour, val.minute, val.second,\n",
    "                    val.microsecond)\n",
    "    elif isinstance(val, DSfloat):\n",
    "        return float(val)\n",
    "    elif isinstance(val, DSdecimal):\n",
    "        return Decimal(val)\n",
    "    elif isinstance(val, IS):\n",
    "        return int(val)\n",
    "    elif isinstance(val, MultiValue):\n",
    "        return tuple([get_native_from_pydicom(el) for el in val])\n",
    "    elif isinstance(val, UID):\n",
    "        return str(val)\n",
    "    elif isinstance(val, (PersonName, str, int, float)):\n",
    "        return val\n",
    "    elif isinstance(val, list):\n",
    "        return tuple(val)\n",
    "    else:\n",
    "        return val        \n",
    "\n",
    "def unpack_pydicom_value(value):\n",
    "    if hasattr(type(value),'__module__') and type(value).__module__.startswith('pydicom'):\n",
    "        return get_native_from_pydicom(value)\n",
    "    elif isinstance(value, list):\n",
    "        return tuple([unpack_pydicom_value(v) for v in value])\n",
    "    elif isinstance(value,OrderedDict):\n",
    "        return tuple( [unpack_pydicom_value(v.value) for v in value.values()] )\n",
    "    elif isinstance(value, dict):\n",
    "        return tuple(value.items())\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "def agg_unique(group):\n",
    "    labels = []\n",
    "    row = []\n",
    "    for (name, col) in group[group.columns[:]].iteritems():\n",
    "        labels.append(name)\n",
    "        row.append(col.dropna().unique())\n",
    "\n",
    "    return pd.DataFrame([row], columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_native_renamed = df_renamed.applymap(lambda x: unpack_pydicom_value(x.value) if pd.notnull(x) else x)\n",
    "\n",
    "group_by_series_keys = ['FileCollectionID',\n",
    "                        'Instance Creation Date',\n",
    "                        'Patient ID',\n",
    "                        'Study Instance UID',\n",
    "                        'Sequence Name',\n",
    "                        'Series Instance UID',\n",
    "                        'Image Type']\n",
    "\n",
    "df_native_grouped = df_native_renamed.groupby(group_by_series_keys)\n",
    "df_native_grouped_unique = df_native_grouped.apply(lambda x: agg_unique(x))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all tags with <= 1 unique values per series in any series\n",
    "\n",
    "df_native_grouped_unique_singleton_cols = df_native_grouped_unique.applymap(lambda x: len(x) <= 1).all()\n",
    "df_native_grouped_unique.drop(\n",
    "    df_native_grouped_unique.columns[~df_native_grouped_unique_singleton_cols], axis=1).applymap(lambda x: x[0] if len(x) == 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df_native_grouped_unique['De-identification Method Code Sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_native_grouped_unique.drop(\n",
    "    df_native_grouped_unique.columns[df_native_grouped_unique_singleton_cols],axis=1).applymap(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all tags with more than a single unique values in at least one series\n",
    "\n",
    "df_native_grouped_unique.drop(\n",
    "    df_native_grouped_unique.columns[df_native_grouped_unique_singleton_cols],axis=1).applymap(lambda x: x[0] if len(x) == 1 else x).drop(['FilePath'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Referenced Image Sequence tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_referenced_uid = df_native_renamed['Referenced Image Sequence'].iloc[0][0][1]\n",
    "\n",
    "# Check if we can find the referenced image sequence\n",
    "print(\"The string length of a Referenced Image Sequence UID is {}: {}.\".format(len(example_referenced_uid), example_referenced_uid))\n",
    "        \n",
    "def get_uid_prefix(uid):\n",
    "    return '.'.join(uid.split('.')[:-1])\n",
    "example_referenced_uid_prefix = get_uid_prefix(example_referenced_uid)\n",
    "print(\"Example referenced UID prefix is {}\".format(example_referenced_uid_prefix))\n",
    "\n",
    "print(\"Share of SOP Instance UIDs with same prefix up to and after last point: {}.\".format(', '.join([\n",
    "    str(100.*\n",
    "    (df_native_renamed['SOP Instance UID'].apply(lambda x: x[:prefix_len] ) == example_referenced_uid[:prefix_len] ).sum()/\n",
    "    df_native_renamed.shape[0]) + \" %\" for prefix_len in range(len(example_referenced_uid_prefix),len(example_referenced_uid_prefix)+10)]) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Referenced Image Sequence Tag identical before last point, afterwards ~random - the prefix 1.2.40.0.13.1 is taken from the dcm4che DICOM implementation, the remainder is a randomly generated UID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of time (Acquisition time, Content time, Trigger time and Nominal interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime  \n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_series_title = \"Group by series UID plot (\" + \\\n",
    "                        \" / \".join([group_by_series_keys[0],\n",
    "                                    group_by_series_keys[1],\n",
    "                                    group_by_series_keys[4],\n",
    "                                    group_by_series_keys[6],\n",
    "                                    'Nominal Interval',\n",
    "                                    'Cardiac Number of Images',\n",
    "                                    '[P-index]'\n",
    "                                  ]) + \\\n",
    "                        \")\"\n",
    "\n",
    "def legend_unique_values(ls):\n",
    "    return str(ls[0]) if len(ls)==1 else \"{}..{}\".format(ls.min(), ls.max()) \n",
    "\n",
    "def group_by_series_legend(name, group, j=None):\n",
    "    nominal_intervals = group['Nominal Interval'].unique() #.apply(lambda x: x.original_string).unique()\n",
    "    cardiac_number_of_images = group['Cardiac Number of Images'].unique() #.apply(lambda x: x.original_string).unique()\n",
    "    return \", \".join([\n",
    "            str(name[0]),\n",
    "            str(name[1]),\n",
    "            name[4],\n",
    "            str(name[6]), \n",
    "            \"NomInt {} ms\".format(legend_unique_values(nominal_intervals)),\n",
    "            \"CardImgs {}\".format(legend_unique_values(cardiac_number_of_images))\n",
    "            ]) + (\", P-{}\".format(j) if j is not None else \"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from inspect import signature\n",
    "\n",
    "# groups = df.\\\n",
    "#     applymap(lambda x: unpack_pydicom_value(x.value) if pd.notnull(x) else x).\\\n",
    "#     groupby(group_by_series_keys)\n",
    "\n",
    "def make_group_by_plot(groups, x_series, x_label, y_series, y_label):    \n",
    "    \n",
    "    ngroups = len(groups.groups)\n",
    "    ncols = 3\n",
    "    nrows = (ngroups+3-1)//ncols    \n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=nrows,ncols=ncols, sharex=True, figsize=(12*ncols,8*nrows))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i, (name, group) in enumerate(groups): \n",
    "\n",
    "        if 'P' not in group['Image Type'].iloc[0]:\n",
    "            ax[i].plot(x_series(group), \n",
    "                       y_series(group), \n",
    "                       'o',  ms=6, linewidth=1, \n",
    "                       label=group_by_series_legend(name,group))\n",
    "        else:        \n",
    "            for j in range(3):\n",
    "                select_subgroup = group['Instance Number'].apply(lambda x: j*group.shape[0]//3 < x and x <= (j+1)*group.shape[0]//3)            \n",
    "                ax[i].plot(x_series(group[select_subgroup]), \n",
    "                           y_series(group[select_subgroup]),\n",
    "                           'o',  ms=6, linewidth=1, \n",
    "                           label=group_by_series_legend(name,group,j))\n",
    "\n",
    "    for a in ax[:ngroups]: \n",
    "        a.set_xlabel(x_label)\n",
    "        a.set_ylabel(y_label)\n",
    "\n",
    "        a.legend(bbox_to_anchor=(-0.3, 1.04), loc=\"lower left\")\n",
    "        a.grid()\n",
    "    fig.suptitle(group_by_series_title)\n",
    "    fig.tight_layout(pad=3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquisition Time vs. Trigger Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquisition_datetime(df):\n",
    "    return df.apply(\n",
    "        lambda x: datetime.datetime(x['Series Date'].year, \n",
    "                                    x['Series Date'].month, \n",
    "                                    x['Series Date'].day, \n",
    "                                    x['Acquisition Time'].hour, \n",
    "                                    x['Acquisition Time'].minute,\n",
    "                                    x['Acquisition Time'].second, \n",
    "                                    x['Acquisition Time'].microsecond, \n",
    "                                    x['Acquisition Time'].tzinfo ), axis=1)\n",
    "\n",
    "def acqusition_datetime_offset(df):\n",
    "    acquisition_datetime_series = acquisition_datetime(df)\n",
    "    return acquisition_datetime_series - acquisition_datetime_series.min()\n",
    "\n",
    "\n",
    "group_x_series = lambda group: group['Trigger Time']\n",
    "x_label = 'Trigger Time [ms]'\n",
    "group_y_series = lambda group: acqusition_datetime_offset(group).apply(lambda x: x.total_seconds()*1e3)+group['Trigger Time'].min()\n",
    "y_label = 'Acquisition Time (offset to min) [ms]'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the Trigger Time and Acquisition time coincide up to an offset for Acquisition time. Also, they do not cover the entire heart cycle but only a bit more than the first half. This could be the case since it may be a prospective hear MRI as explained [here](http://mriquestions.com/gating-parameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger time vs. nominal interval\n",
    "pd.concat([df_native_grouped_unique[['Trigger Time']].applymap(lambda x: np.max(x)), df_native_grouped_unique[['Nominal Interval']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Time vs. Trigger Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_datetime(df):\n",
    "    return df.apply(\n",
    "        lambda x: datetime.datetime(x['Series Date'].year, \n",
    "                                    x['Series Date'].month, \n",
    "                                    x['Series Date'].day, \n",
    "                                    x['Content Time'].hour, \n",
    "                                    x['Content Time'].minute,\n",
    "                                    x['Content Time'].second, \n",
    "                                    x['Content Time'].microsecond, \n",
    "                                    x['Content Time'].tzinfo ), axis=1)\n",
    "\n",
    "def content_datetime_offset(df):\n",
    "    content_datetime_series = content_datetime(df)\n",
    "    return content_datetime_series - content_datetime_series.min()\n",
    "\n",
    "group_x_series = lambda group: group['Trigger Time']\n",
    "x_label = 'Trigger Time [ms]'\n",
    "group_y_series = lambda group: content_datetime_offset(group).apply(lambda x: x.total_seconds()*1e3)+group['Trigger Time'].min()\n",
    "y_label = 'Content Time (offset to min) [ms]'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that the Content Time refers to the time when the MRI signal was recorded - the entire sequence is spread over more than 40 seconds. No structure apparent from these plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger Time vs. Instance Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_x_series = lambda group: group['Instance Number'] \n",
    "x_label = 'Instance Number'\n",
    "group_y_series = lambda group:  group['Trigger Time'] \n",
    "y_label = 'Trigger Time [ms]'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots clearly suggest that the P-series contain 3 sub-series that are sorted according to instance number (the visible plateau is due to multiple slices per fixed time being recorded one after another). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Time vs. Instance Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_x_series = lambda group: group['Instance Number']\n",
    "x_label = 'Instance Number'\n",
    "group_y_series = lambda group: content_datetime_offset(group).apply(lambda x: x.total_seconds()*1e3)+group['Trigger Time'].min()\n",
    "y_label = 'Content Time (offset to min) [ms]'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No correlation between content time and instance number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Creation Time vs. Instance Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_creation_datetime(df):\n",
    "    return df.apply(\n",
    "        lambda x: datetime.datetime(x['Series Date'].year, \n",
    "                                    x['Series Date'].month, \n",
    "                                    x['Series Date'].day, \n",
    "                                    x['Instance Creation Time'].hour, \n",
    "                                    x['Instance Creation Time'].minute,\n",
    "                                    x['Instance Creation Time'].second, \n",
    "                                    x['Instance Creation Time'].microsecond, \n",
    "                                    x['Instance Creation Time'].tzinfo ), axis=1)\n",
    "\n",
    "def instance_creation_datetime_offset(df):\n",
    "    content_datetime_series = content_datetime(df)\n",
    "    return content_datetime_series - content_datetime_series.min()\n",
    "\n",
    "group_x_series = lambda group: group['Instance Number']\n",
    "x_label = 'Instance Number'\n",
    "group_y_series = lambda group: instance_creation_datetime_offset(group).apply(lambda x: x.total_seconds()*1e3)+group['Trigger Time'].min()\n",
    "y_label = 'Instance Creation Time (offset to min) [ms]'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOP Instance UID vs. Instance Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_x_series = lambda group: group['Instance Number'] \n",
    "x_label = 'Instance Number'\n",
    "group_y_series = lambda group:  group['SOP Instance UID'].apply(lambda x: int(x.split('.')[-1]))\n",
    "y_label = 'SOP Instance UID'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique SOP Instance UID prefixes\n",
    "df_native_renamed['SOP Instance UID'].apply(lambda x: get_uid_prefix(x)).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SOP Instance UID seems to be an Implementation label for dcm4che followed by a randomly generated UID with no correlation with Instance Number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice Location vs. Instance Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_x_series = lambda group: group['Instance Number']\n",
    "x_label = 'Instance Number'\n",
    "group_y_series = lambda group: group['Slice Location']\n",
    "y_label = 'Slice Location'\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice Location (col x row coord sys) - Slice Location (DICOM) vs. Instance Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_x_series = lambda group: group['Instance Number'] #.apply(lambda x: float(x.original_string))\n",
    "x_label = 'Instance Number'\n",
    "\n",
    "def slice_location_image_coord_sys(group):\n",
    "    seq_origin = np.array(group.loc[group['Instance Number'].idxmin(),'Image Position (Patient)'])    \n",
    "    \n",
    "    orientation = group['Image Orientation (Patient)'].unique()\n",
    "    assert len(orientation) == 1\n",
    "\n",
    "    row_unit = np.array(orientation[0][:3])\n",
    "    col_unit = np.array(orientation[0][3:])\n",
    "    ort_unit = np.cross(col_unit, row_unit)\n",
    "\n",
    "    return group['Image Position (Patient)'].apply(lambda x: np.dot(ort_unit, np.array(x)-seq_origin) )\n",
    "\n",
    "def slice_location_diff(group):\n",
    "    return slice_location_image_coord_sys(group) - group['Slice Location'].apply(lambda x: np.array(x))\n",
    "\n",
    "group_y_series = slice_location_diff\n",
    "y_label = 'Slice Location (col x row) - Slice Location (DICOM)'\n",
    "\n",
    "make_group_by_plot(df_native_grouped, group_x_series, x_label, group_y_series, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinate system of the DICOM images produces the same alignment (with \"Slice Location\" as z-axis) when taking the column direction as the primary and row-direction as the secondary axis (i.e. x runs along the columns, y along the rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check orthogonality of image planes and relative image offset vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (name, group) in enumerate(pd.concat([df_renamed, instance_creation_datetime], axis=1).\\\n",
    "#     applymap(lambda x: unpack_pydicom_value(x.value) if pd.notnull(x) else x).\\\n",
    "#     groupby(['FileCollectionID', 'Instance Creation Date', 'Patient ID', 'Study Instance UID', 'Series Instance UID', 'Image Type', 'Sequence Name'])): # group['Instance Creation Date'].iloc[0]\n",
    "\n",
    "def print_if_above_numeric_tol(name, value, tol=1e-11):\n",
    "    if value > tol:\n",
    "        print(\"    FAIL: {} = {} exceeds numeric tolerance \".format(name, value))\n",
    "    else:\n",
    "        print(\"    OK:   {} = {} \".format(name, value))\n",
    "            \n",
    "\n",
    "for i, (name, group) in enumerate(df_native_grouped):\n",
    "\n",
    "    orientation = group['Image Orientation (Patient)'].unique()\n",
    "    assert len(orientation) == 1\n",
    "\n",
    "    row_unit = np.array(orientation[0][:3])\n",
    "    col_unit = np.array(orientation[0][3:])\n",
    "    ort_unit = np.cross(col_unit, row_unit)\n",
    "    \n",
    "\n",
    "    if 'P' not in group['Image Type'].iloc[0]:\n",
    "\n",
    "        print(name[0],name[1],name[4], name[6])\n",
    "\n",
    "        positions = group.sort_values('Instance Number')['Image Position (Patient)'].apply(lambda x: np.array(x))    \n",
    "        #pprint((positions.iloc[1:] - positions[:-1]).apply(lambda x: np.linalg.norm(x)).max())\n",
    "        if len(positions) > 1:\n",
    "            print_if_above_numeric_tol(\"max(dot(col_unit, patient_position_offset))\", np.abs(np.dot( np.vstack(positions.to_numpy()[1:] - positions.to_numpy()[:-1]), col_unit) ).max() )\n",
    "            print_if_above_numeric_tol(\"max(dot(row_unit, patient_position_offset))\", np.abs(np.dot( np.vstack(positions.to_numpy()[1:] - positions.to_numpy()[:-1]), row_unit) ).max() )\n",
    "\n",
    "    else:\n",
    "        for j in range(3):\n",
    "\n",
    "            print(name[0], name[1], name[4], name[6], j)\n",
    "\n",
    "            select_subgroup = group['Instance Number'].apply(lambda x: j*group.shape[0]//3 < x and x <= (j+1)*group.shape[0]//3)            \n",
    "\n",
    "            positions = group[select_subgroup].sort_values('Instance Number')['Image Position (Patient)'].apply(lambda x: np.array(x))\n",
    "            if len(positions) > 1:\n",
    "                print_if_above_numeric_tol(\"max(dot(col_unit, patient_position_offset))\", np.abs(np.dot( np.vstack(positions.to_numpy()[1:] - positions.to_numpy()[:-1]), col_unit) ).max() )\n",
    "                print_if_above_numeric_tol(\"max(dot(row_unit, patient_position_offset))\", np.abs(np.dot( np.vstack(positions.to_numpy()[1:] - positions.to_numpy()[:-1]), row_unit) ).max() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image planes are orthogonal to Image Position (Patient) offset vector (note that the calculation wraps around time slice edges, i.e. it computes offsets between last and first Image Position (Patient), which, however, does not affect the conclusion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# - Check that Cardiac number of images coincides with number of times (trigger/acquisition)\n",
    "# - number of slice locations coincides with number of image position patients\n",
    "# - and that both together with Rows and Columns explain the number of instances\n",
    "# - statistics over unique values per series and conversion to HPC-PREDICT-IO/create MRI writer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion to HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DICOM Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_native_grouped_unique[['FilePath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found sequence names {} (only processing fl3d1).\".format(df_native_renamed['Sequence Name'].unique()))\n",
    "\n",
    "group_by_series_keys = ['FileCollectionID', 'Instance Creation Date', 'Patient ID', 'Study Instance UID', 'Sequence Name'] #, 'Series Instance UID', 'Image Type']\n",
    "df_flash_grouped = df_native_renamed[df_native_renamed['Sequence Name'] == 'fl3d1'].groupby(group_by_series_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_mris = []\n",
    "\n",
    "def extract_unique_value(group, col):\n",
    "    value = group[col].unique()\n",
    "    assert len(value) == 1; \n",
    "    return value[0]\n",
    "\n",
    "for (name, group) in df_flash_grouped:\n",
    "\n",
    "    magnitude = None\n",
    "    phase = []\n",
    "    \n",
    "    for (series_name, series_group) in group.groupby(['Series Instance UID', 'Image Type']):\n",
    "        if 'M' in series_name[1]:\n",
    "            magnitude = series_group.sort_values('Instance Number')\n",
    "        elif 'P' in series_name[1]:\n",
    "            phase_group = series_group.sort_values('Instance Number')\n",
    "            phase = [ phase_group[ phase_group['Instance Number'].apply(\n",
    "                lambda x: j*phase_group.shape[0]//3 < x and x <= (j+1)*phase_group.shape[0]//3) ] \n",
    "                     for j in range(3) ]           \n",
    "\n",
    "    num_rows = extract_unique_value(group, 'Rows')\n",
    "    num_cols = extract_unique_value(group, 'Columns')\n",
    "    num_slice_locations = len(group['Slice Location'].unique()) # FIXME: sanity check!\n",
    "    cardiac_number_of_images = extract_unique_value(group, 'Cardiac Number of Images')\n",
    "    \n",
    "    assert magnitude.shape[0] == num_slice_locations*cardiac_number_of_images\n",
    "    for j in range(3):\n",
    "        assert phase[j].shape[0] == num_slice_locations*cardiac_number_of_images\n",
    "\n",
    "    orientation = extract_unique_value(group, 'Image Orientation (Patient)')\n",
    "    row_unit = np.array(orientation[:3])\n",
    "    col_unit = np.array(orientation[3:])\n",
    "    ort_unit = np.cross(col_unit, row_unit)\n",
    "    assert (np.linalg.norm(ort_unit) - 1.) < 1e-12\n",
    "\n",
    "    magnitude_positions = magnitude['Image Position (Patient)'].apply(lambda x: np.array(x))    \n",
    "    magnitude_position_origin = np.array(magnitude.loc[magnitude['Instance Number'].idxmin(),'Image Position (Patient)'])    \n",
    "    magnitude_slice_position_shift = magnitude_positions.apply(lambda x: np.dot(ort_unit,x - magnitude_position_origin)) - magnitude['Slice Location']    \n",
    "    assert (magnitude_slice_position_shift - magnitude_slice_position_shift.mean()).abs().max() < 1e-11\n",
    "    \n",
    "    slice_locations = magnitude['Slice Location'][:num_slice_locations].values # FIXME: sanity check!\n",
    "    assert len(np.unique(slice_locations)) == num_slice_locations\n",
    "#     magnitude_slice_locations = magnitude['Slice Location'][:num_slice_locations].values    \n",
    "    magnitude_trigger_times = magnitude['Trigger Time'][::num_slice_locations].values\n",
    "    assert len(np.unique(magnitude_trigger_times)) == cardiac_number_of_images\n",
    "    magnitude_values = np.ndarray(shape=(num_rows, num_cols, num_slice_locations, cardiac_number_of_images))\n",
    "    # TODO: Check correctness of pixel value reading\n",
    "    for t,_ in enumerate(magnitude_trigger_times):\n",
    "        for z,_ in enumerate(slice_locations): # magnitude_slice_locations\n",
    "            img_row = magnitude[magnitude['Instance Number'] == t*num_slice_locations + z + 1]; assert len(img_row) == 1; img_row = img_row.iloc[0]\n",
    "            dcm_img = pydicom.dcmread(img_row['FilePath'])\n",
    "            magnitude_values[:,:,z,t] = dcm_img.pixel_array # no rescale properties\n",
    "   \n",
    "\n",
    "    phase_positions = []\n",
    "    phase_slice_locations = []\n",
    "    phase_trigger_times = []\n",
    "    phase_values = np.ndarray(shape=(num_rows, num_cols, num_slice_locations, cardiac_number_of_images, 3))\n",
    "    for j in range(3):\n",
    "        # If this does not work can validate the phase z-coordinates here as well\n",
    "        assert (slice_locations == phase[j]['Slice Location'][:num_slice_locations].values).all()\n",
    "\n",
    "#         phase_slice_locations.append( phase[j]['Slice Location'][:num_slice_locations].values )    \n",
    "        phase_trigger_times.append( phase[j]['Trigger Time'][::num_slice_locations].values )\n",
    "        print(\"Trigger time offsets of phase[{}] relative to magnitude in {} ms\".format(j, np.unique(phase_trigger_times[j]-magnitude_trigger_times)), flush=True)\n",
    "        for t,_ in enumerate(phase_trigger_times[j]):\n",
    "            for z,_ in enumerate(slice_locations): # phase_slice_locations[j]\n",
    "                img_row = phase[j][phase[j]['Instance Number'] == j*cardiac_number_of_images*num_slice_locations + t*num_slice_locations + z + 1]; assert len(img_row) == 1; img_row = img_row.iloc[0]\n",
    "                dcm_img = pydicom.dcmread(img_row['FilePath'])\n",
    "                phase_values[:,:,z,t,j] = dcm_img.RescaleSlope*dcm_img.pixel_array+dcm_img.RescaleIntercept\n",
    "\n",
    "    \n",
    "    #check for uniqueness!\n",
    "    pixel_spacing = extract_unique_value(group, 'Pixel Spacing')\n",
    "    geometry = [pixel_spacing[0]*(np.arange(0,num_rows)+0.5),\n",
    "                pixel_spacing[1]*(np.arange(0,num_cols)+0.5),\n",
    "                slice_locations]\n",
    "\n",
    "    #check for uniqueness!\n",
    "    #group['Nominal Interval'].unique()  #.apply(lambda x: agg_unique(x))\n",
    "    heart_cycle_period = group['Nominal Interval'].mean()\n",
    "    \n",
    "    print(\"Writing {} to HDF5...\".format(name))\n",
    "    flow_mris.append({\n",
    "        \"study_instance_uid\": name[3],\n",
    "        \"sequence_name\": name[4],\n",
    "        \"cardiac_number_of_images\": cardiac_number_of_images,\n",
    "        \"num_slice_locations\": num_slice_locations,\n",
    "        \"heart_cycle_period\": heart_cycle_period,\n",
    "        \"geometry\": geometry,\n",
    "        \"magnitude_trigger_times\": magnitude_trigger_times,\n",
    "        \"magnitude_values\": magnitude_values, #\"phase_slice_locations\": phase_slice_locations,\n",
    "        \"phase_trigger_times\": phase_trigger_times,\n",
    "        \"phase_values\": phase_values,        \n",
    "    })\n",
    "## df_native_grouped_unique = df_native_grouped.apply(lambda x: agg_unique(x))   # \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not '/home/lukasd/src/review/hpc-predict-io/python' in sys.path:\n",
    "    sys.path.append('/home/lukasd/src/review/hpc-predict-io/python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mr_io import FlowMRI\n",
    "\n",
    "hpc_predict_mri = FlowMRI(geometry=geometry, \n",
    "                          time=magnitude_trigger_times, # FIXME: phase times!\n",
    "                          time_heart_cycle_period=heart_cycle_period, \n",
    "                          intensity=magnitude_values, \n",
    "                          velocity_mean=phase_values, \n",
    "                          velocity_cov=np.zeros(shape=phase_values.shape+(3,)))\n",
    "# FlowMRI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"{}-{}-{}.h5\".format(name[0], name[1], name[3])\n",
    "hpc_predict_mri.write_hdf5(filename)\n",
    "\n",
    "read_result = FlowMRI.read_hdf5(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure, ax = plt.subplots(ncols=4, sharex=True, figsize=(12*4,8*1))\n",
    "ax[0].hist(flow_mris[0]['magnitude_values'].flatten(), bins=50, label='magnitude')\n",
    "ax[0].hist(hpc_predict_mri.intensity.flatten(), bins=50, label='magnitude-hpc-predict') # Todo: make this semitransparent\n",
    "for j in range(3):\n",
    "    ax[j+1].hist(flow_mris[0]['phase_values'][:,:,:,:,j].flatten(), bins=50, label='phase-{}'.format(j))\n",
    "    ax[j+1].hist(hpc_predict_mri.velocity_mean[:,:,:,:,j].flatten(), bins=50, label='phase-{}-hpc-predict'.format(j))\n",
    "figure.legend()\n",
    "figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good on samples. Phase/velocity units remain to be determined. According to analysis below, all sequences are either fl3d1 or fl3d1_2 in the dataset. The code can be moved to a script that does the conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype of DICOM writer (immature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine target folder\n",
    "\n",
    "# slice_location = geometry[2]\n",
    "# calculate image position patient from old_image_position_origin + (geometry[2] - slice_location[0])*ort_unit\n",
    "# Pixel Spacing = (geometry[i][-1] - geometry[i][1])/(len(geometry[i])-1)\n",
    "# instance number: magnitude: t*num_slice_locations + z + 1, \n",
    "#                  phase: j*cardiac_number_of_images*num_slice_locations + t*num_slice_locations + z + 1\n",
    "# trigger times\n",
    "# Rows = len(geometry[0])\n",
    "# Columns = = len(geometry[0])\n",
    "# Rescale Intercept/Slope ? Intercept = -max(abs(velocity_mean)), slope = 2*abs(Intercept)/2*Bits Stored(?)\n",
    "# skip all other tags with multiplicity > 1 such as acquisition/content time (make up SOP Class UID etc. with uid.generate_uid)\n",
    "# for those with multiplicity 1 select which ones to add \n",
    "#\n",
    "# Create tags to annotate HPC-PREDICT information, such as code that was used to produce the data\n",
    "#\n",
    "# Question: how do we add overlays? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_native_grouped_unique_flow = df_native_grouped_unique[(df_native_grouped_unique['Study Instance UID'] == flow_mris[1]['study_instance_uid']) & (df_native_grouped_unique['Sequence Name'] == flow_mris[1]['sequence_name'])]\n",
    "df_native_grouped_unique_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_native_grouped_unique_flow_reindexed = df_native_grouped_unique_flow.set_index(df_native_grouped_unique_flow['Image Type'].apply(lambda x: x[0][2]))\n",
    "# df_native_grouped_unique_flow_reindexed.loc['M']['FilePath'][0]\n",
    "\n",
    "singleton_labels = df_native_grouped_unique_flow.drop(df_native_grouped_unique_flow.columns[~df_native_grouped_unique_flow.applymap(lambda x: not(hasattr(x,'__len__') and len(x) == 1) ).all()], axis=1) #.columns.values\n",
    "singleton_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_series_keys = ['FileCollectionID', 'Instance Creation Date', 'Patient ID', 'Study Instance UID', 'Sequence Name', 'Series Instance UID', 'Image Type']\n",
    "# df.groupby(group_by_series_keys)\n",
    "\n",
    "# Get study instance UID, convert to pandas table, compute unique tags by base tag address, iterate over tags - for unique copy them to new dataset except if should be included (by second table tag -> name), for non-unique, replace them appropriately (name look-up, etc.) \n",
    "\n",
    "tag_to_name = df.apply( lambda c: c.dropna().apply(lambda x: x.name).unique(), axis=0).transpose()\n",
    "name_to_tag = tag_to_name.reset_index().rename(columns={0: 'name', 'index': 'tag'}).set_index('name')\n",
    "\n",
    "tag_to_name[0]\n",
    "name_to_tag['tag']['Study Instance UID']\n",
    "#df[df.apply(lambda row: row['Study Instance UID'] == study_instance_uid and row['Sequence Name'] == sequence_name, axis=1)]\n",
    "\n",
    "# df_hashified = df.applymap(lambda x: unpack_pydicom_value(x.value) if pd.notnull(x) else x)\n",
    "# df_tag_to_name = df.applymap(lambda x: x.name if pd.notnull(x) else x)\n",
    "# df_hashified.groupby(group_by_series_keys)\n",
    "\n",
    "# # df_native_grouped = df_native_renamed.groupby(group_by_series_keys)\n",
    "# # df_native_grouped.apply(lambda x: agg_unique(x))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicom_img[name_to_tag['tag']['SOP Class UID']]\n",
    "pydicom.dcmread(df_native_grouped_unique_flow_reindexed.loc['M']['FilePath'][0]).file_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_native_grouped_unique_flow_reindexed.loc['M','Specific Character Set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_meta = FileMetaDataset\n",
    "# file_meta = pydicom.dataset.FileMetaDataset()\n",
    "# file_meta.MediaStorageSOPClassUID = dicom_img.file_meta[0x00020002].value\n",
    "# file_meta.ImplementationClassUID = PYDICOM_IMPLEMENTATION_UID\n",
    "\n",
    "# First get the study (flow MRI needs to describe data it comes from...)\n",
    "\n",
    "# for series in source study, open first image as dicom_img\n",
    "dcm_img = pydicom.dcmread(df_native_grouped_unique_flow_reindexed.loc['M']['FilePath'][0])\n",
    "# create file_meta_ds/file_ds\n",
    "ds = pydicom.dataset.Dataset()\n",
    "\n",
    "print(\"### Simulating writing of DICOM ###\")\n",
    "for el in dcm_img: # also include dcm_img.file_meta later\n",
    "    print(el.name, flush=True)\n",
    "    if el.name == 'Pixel Data':\n",
    "        print(\"Writing Pixel Data\") # replace dcm-che prefix by pydicom prefix        \n",
    "    elif len(df_native_grouped_unique_flow_reindexed.loc['M', el.name]) == 1: # FIXME: use el.tag instead of el.name (compute unique values by tags)\n",
    "        if not el.name in ['Referenced Image Sequence', 'Study Instance UID', 'Series Instance UID', 'Rows', 'Columns', 'Pixel Spacing']: # more? for UIDs can just replace dcm-che prefix by that of pydicom implementation\n",
    "            print(\"Copying unique element {} \".format(el.name)) # replace dcm-che prefix by pydicom prefix\n",
    "#             file_ds.add(el.tag, el.VR, el.value) # module modifications...\n",
    "        else: # special treatment of el.tag\n",
    "            print(\"Special treatment for {} \".format(el.name)) # replace dcm-che prefix by pydicom prefix\n",
    "    else: # probably one of the tags that has to be computed... (see above for the manual)\n",
    "        print(\"Element {} has different values\".format(el.name))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
